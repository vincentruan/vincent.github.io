{"meta":{"title":"星辰大海","subtitle":"My Conquest Is the Sea of Stars.","description":"The King is dead, long live the King!","author":"vincent","url":"https://vincentruan.github.io"},"pages":[{"title":"文章分类","date":"2018-05-26T09:18:05.000Z","updated":"2018-05-26T09:19:34.334Z","comments":false,"path":"categories/index.html","permalink":"https://vincentruan.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-05-26T09:22:14.000Z","updated":"2018-05-26T09:22:57.773Z","comments":false,"path":"tags/index.html","permalink":"https://vincentruan.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"memcached与redis实现的对比","slug":"memcached与redis实现的对比","date":"2019-12-29T07:03:32.000Z","updated":"2019-12-29T07:20:02.350Z","comments":true,"path":"2019/12/29/memcached与redis实现的对比/","link":"","permalink":"https://vincentruan.github.io/2019/12/29/memcached与redis实现的对比/","excerpt":"","text":"​ memcached和redis，作为近些年最常用的缓存服务器，相信大家对它们再熟悉不过了。前两年还在学校时，我曾经读过它们的主要源码，如今写篇笔记从个人角度简单对比一下它们的实现方式，权当做复习，有理解错误之处，欢迎指正。 ​ 文中使用的架构类的图片大多来自于网络，有部分图与最新实现有出入，文中已经指出。 1. 综述​ 读一个软件的源码，首先要弄懂软件是用作干什么的，那memcached和redis是干啥的？众所周知，数据一般会放在数据库中，但是查询数据会相对比较慢，特别是用户很多时，频繁的查询，需要耗费大量的时间。怎么办呢？数据放在哪里查询快？那肯定是内存中。memcached和redis就是将数据存储在内存中，按照key-value的方式查询，可以大幅度提高效率。所以一般它们都用做缓存服务器，缓存常用的数据，需要查询的时候，直接从它们那儿获取，减少查询数据库的次数，提高查询效率。 2. 服务方式memcached和redis怎么提供服务呢？它们是独立的进程，需要的话，还可以让他们变成daemon进程，所以我们的用户进程要使用memcached和redis的服务的话，就需要进程间通信了。考虑到用户进程和memcached和redis不一定在同一台机器上，所以还需要支持网络间通信。因此，memcached和redis自己本身就是网络服务器，用户进程通过与他们通过网络来传输数据，显然最简单和最常用的就是使用tcp连接了。另外，memcached和redis都支持udp协议。而且当用户进程和memcached和redis在同一机器时，还可以使用unix域套接字通信。 3. 事件模型下面开始讲他们具体是怎么实现的了。首先来看一下它们的事件模型。 ​ 自从epoll出来以后，几乎所有的网络服务器全都抛弃select和poll，换成了epoll。redis也一样，只不多它还提供对select和poll的支持，可以自己配置使用哪一个，但是一般都是用epoll。另外针对BSD，还支持使用kqueue。而memcached是基于libevent的，不过libevent底层也是使用epoll的，所以可以认为它们都是使用epoll。epoll的特性这里就不介绍了，网上介绍文章很多。 它们都使用epoll来做事件循环，不过redis是单线程的服务器（redis也是多线程的，只不过除了主线程以外，其他线程没有event loop，只是会进行一些后台存储工作），而memcached是多线程的。 redis的事件模型很简单，只有一个event loop，是简单的reactor实现。不过redis事件模型中有一个亮点，我们知道epoll是针对fd的，它返回的就绪事件也是只有fd，redis里面的fd就是服务器与客户端连接的socket的fd，但是处理的时候，需要根据这个fd找到具体的客户端的信息，怎么找呢？通常的处理方式就是用红黑树将fd与客户端信息保存起来，通过fd查找，效率是lgn。不过redis比较特殊，redis的客户端的数量上限可以设置，即可以知道同一时刻，redis所打开的fd的上限，而我们知道，进程的fd在同一时刻是不会重复的（fd只有关闭后才能复用），所以redis使用一个数组，将fd作为数组的下标，数组的元素就是客户端的信息，这样，直接通过fd就能定位客户端信息，查找效率是O(1)，还省去了复杂的红黑树的实现（我曾经用c写一个网络服务器，就因为要保持fd和connect对应关系，不想自己写红黑树，然后用了STL里面的set，导致项目变成了c++的，最后项目使用g++编译，这事我不说谁知道？）。显然这种方式只能针对connection数量上限已确定，并且不是太大的网络服务器，像nginx这种http服务器就不适用，nginx就是自己写了红黑树。 而memcached是多线程的，使用master-worker的方式，主线程监听端口，建立连接，然后顺序分配给各个工作线程。每一个从线程都有一个event loop，它们服务不同的客户端。master线程和worker线程之间使用管道通信，每一个工作线程都会创建一个管道，然后保存写端和读端，并且将读端加入event loop，监听可读事件。同时，每个从线程都有一个就绪连接队列，主线程连接连接后，将连接的item放入这个队列，然后往该线程的管道的写端写入一个connect命令，这样event loop中加入的管道读端就会就绪，从线程读取命令，解析命令发现是有连接，然后就会去自己的就绪队列中获取连接，并进行处理。多线程的优势就是可以充分发挥多核的优势，不过编写程序麻烦一点，memcached里面就有各种锁和条件变量来进行线程同步。 4. 内存分配memcached和redis的核心任务都是在内存中操作数据，内存管理自然是核心的内容。 首先看看他们的内存分配方式。memcached是有自己得内存池的，即预先分配一大块内存，然后接下来分配内存就从内存池中分配，这样可以减少内存分配的次数，提高效率，这也是大部分网络服务器的实现方式，只不过各个内存池的管理方式根据具体情况而不同。而redis没有自己得内存池，而是直接使用时分配，即什么时候需要什么时候分配，内存管理的事交给内核，自己只负责取和释放（redis既是单线程，又没有自己的内存池，是不是感觉实现的太简单了？那是因为它的重点都放在数据库模块了）。不过redis支持使用tcmalloc来替换glibc的malloc，前者是google的产品，比glibc的malloc快。 ​ 由于redis没有自己的内存池，所以内存申请和释放的管理就简单很多，直接malloc和free即可，十分方便。而memcached是支持内存池的，所以内存申请是从内存池中获取，而free也是还给内存池，所以需要很多额外的管理操作，实现起来麻烦很多，具体的会在后面memcached的slab机制讲解中分析。 5. 数据库实现接下来看看他们的最核心内容，各自数据库的实现。 5.1 memcached数据库实现memcached只支持key-value，即只能一个key对于一个value。它的数据在内存中也是这样以key-value对的方式存储，它使用slab机制。 首先看memcached是如何存储数据的，即存储key-value对。如下图，每一个key-value对都存储在一个item结构中，包含了相关的属性和key和value的值。 ​ item是保存key-value对的，当item多的时候，怎么查找特定的item是个问题。所以memcached维护了一个hash表，它用于快速查找item。hash表适用开链法（与redis一样）解决键的冲突，每一个hash表的桶里面存储了一个链表，链表节点就是item的指针，如上图中的h_next就是指桶里面的链表的下一个节点。 hash表支持扩容（item的数量是桶的数量的1.5以上时扩容），有一个primary_hashtable，还有一个old_hashtable，其中正常适用primary_hashtable，但是扩容的时候，将old_hashtable = primary_hashtable，然后primary_hashtable设置为新申请的hash表（桶的数量乘以2），然后依次将old_hashtable 里面的数据往新的hash表里面移动，并用一个变量expand_bucket记录以及移动了多少个桶，移动完成后，再free原来的old_hashtable 即可（redis也是有两个hash表，也是移动，不过不是后台线程完成，而是每次移动一个桶）。扩容的操作，专门有一个后台扩容的线程来完成，需要扩容的时候，使用条件变量通知它，完成扩容后，它又考试阻塞等待扩容的条件变量。这样在扩容的时候，查找一个item可能会在primary_hashtable和old_hashtable的任意一个中，需要根据比较它的桶的位置和expand_bucket的大小来比较确定它在哪个表里。 item是从哪里分配的呢？从slab中。如下图，memcached有很多slabclass，它们管理slab，每一个slab其实是trunk的集合，真正的item是在trunk中分配的，一个trunk分配一个item。一个slab中的trunk的大小一样，不同的slab，trunk的大小按比例递增，需要新申请一个item的时候，根据它的大小来选择trunk，规则是比它大的最小的那个trunk。这样，不同大小的item就分配在不同的slab中，归不同的slabclass管理。 这样的缺点是会有部分内存浪费，因为一个trunk可能比item大，如图2，分配100B的item的时候，选择112的trunk，但是会有12B的浪费，这部分内存资源没有使用。 如上图，整个构造就是这样，slabclass管理slab，一个slabclass有一个slab_list，可以管理多个slab，同一个slabclass中的slab的trunk大小都一样。slabclass有一个指针slot，保存了未分配的item已经被free掉的item（不是真的free内存，只是不用了而已），有item不用的时候，就放入slot的头部，这样每次需要在当前slab中分配item的时候，直接取slot取即可，不用管item是未分配过的还是被释放掉的。 然后，每一个slabclass对应一个链表，有head数组和tail数组，它们分别保存了链表的头节点和尾节点。链表中的节点就是改slabclass所分配的item，新分配的放在头部，链表越往后的item，表示它已经很久没有被使用了。当slabclass的内存不足，需要删除一些过期item的时候，就可以从链表的尾部开始删除，没错，这个链表就是为了实现LRU。光靠它还不行，因为链表的查询是O（n）的，所以定位item的时候，使用hash表，这已经有了，所有分配的item已经在hash表中了，所以，hash用于查找item，然后链表有用存储item的最近使用顺序，这也是lru的标准实现方法。 每次需要新分配item的时候，找到slabclass对于的链表，从尾部往前找，看item是否已经过期，过期的话，直接就用这个过期的item当做新的item。没有过期的，则需要从slab中分配trunk，如果slab用完了，则需要往slabclass中添加slab了。 memcached支持设置过期时间，即expire time，但是内部并不定期检查数据是否过期，而是客户进程使用该数据的时候，memcached会检查expire time，如果过期，直接返回错误。这样的优点是，不需要额外的cpu来进行expire time的检查，缺点是有可能过期数据很久不被使用，则一直没有被释放，占用内存。 memcached是多线程的，而且只维护了一个数据库，所以可能有多个客户进程操作同一个数据，这就有可能产生问题。比如，A已经把数据更改了，然后B也更改了改数据，那么A的操作就被覆盖了，而可能A不知道，A任务数据现在的状态时他改完后的那个值，这样就可能产生问题。为了解决这个问题，memcached使用了CAS协议，简单说就是item保存一个64位的unsigned int值，标记数据的版本，每更新一次（数据值有修改），版本号增加，然后每次对数据进行更改操作，需要比对客户进程传来的版本号和服务器这边item的版本号是否一致，一致则可进行更改操作，否则提示脏数据。 以上就是memcached如何实现一个key-value的数据库的介绍。 5.2 redis数据库实现首先redis数据库的功能强大一些，因为不像memcached只支持保存字符串，redis支持string， list， set，sorted set，hash table 5种数据结构。例如存储一个人的信息就可以使用hash table，用人的名字做key，然后name super， age 24， 通过key 和 name，就可以取到名字super，或者通过key和age，就可以取到年龄24。这样，当只需要取得age的时候，不需要把人的整个信息取回来，然后从里面找age，直接获取age即可，高效方便。 为了实现这些数据结构，redis定义了抽象的对象redis object，如下图。每一个对象有类型，一共5种：字符串，链表，集合，有序集合，哈希表。 同时，为了提高效率，redis为每种类型准备了多种实现方式，根据特定的场景来选择合适的实现方式，encoding就是表示对象的实现方式的。然后还有记录了对象的lru，即上次被访问的时间，同时在redis 服务器中会记录一个当前的时间（近似值，因为这个时间只是每隔一定时间，服务器进行自动维护的时候才更新），它们两个只差就可以计算出对象多久没有被访问了。 然后redis object中还有引用计数，这是为了共享对象，然后确定对象的删除时间用的。最后使用一个void*指针来指向对象的真正内容。正式由于使用了抽象redis object，使得数据库操作数据时方便很多，全部统一使用redis object对象即可，需要区分对象类型的时候，再根据type来判断。而且正式由于采用了这种面向对象的方法，让redis的代码看起来很像c++代码，其实全是用c写的。 12345//#define REDIS_STRING 0 // 字符串类型//#define REDIS_LIST 1 // 链表类型//#define REDIS_SET 2 // 集合类型(无序的)，可以求差集，并集等//#define REDIS_ZSET 3 // 有序的集合类型//#define REDIS_HASH 4 // 哈希类型//#define REDIS_ENCODING_RAW 0 /* Raw representation */ //raw 未加工//#define REDIS_ENCODING_INT 1 /* Encoded as integer *///#define REDIS_ENCODING_HT 2 /* Encoded as hash table *///#define REDIS_ENCODING_ZIPMAP 3 /* Encoded as zipmap *///#define REDIS_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list *///#define REDIS_ENCODING_ZIPLIST 5 /* Encoded as ziplist *///#define REDIS_ENCODING_INTSET 6 /* Encoded as intset *///#define REDIS_ENCODING_SKIPLIST 7 /* Encoded as skiplist *///#define REDIS_ENCODING_EMBSTR 8 /* Embedded sds string encoding */typedef struct redisObject &#123; unsigned type:4; // 对象的类型，包括 /* Object types */unsigned encoding:4; // 底部为了节省空间，一种type的数据，// 可 以采用不同的存储方式unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */int refcount; // 引用计数void *ptr;&#125; robj; 说到底redis还是一个key-value的数据库，不管它支持多少种数据结构，最终存储的还是以key-value的方式，只不过value可以是链表，set，sorted set，hash table等。和memcached一样，所有的key都是string，而set，sorted set，hash table等具体存储的时候也用到了string。 而c没有现成的string，所以redis的首要任务就是实现一个string，取名叫sds（simple dynamic string），如下的代码， 非常简单的一个结构体，len存储改string的内存总长度，free表示还有多少字节没有使用，而buf存储具体的数据，显然len-free就是目前字符串的长度。 12345struct sdshdr &#123; int len; int free; char buf[];&#125;; 字符串解决了，所有的key都存成sds就行了，那么key和value怎么关联呢？key-value的格式在脚本语言中很好处理，直接使用字典即可，C没有字典，怎么办呢？自己写一个呗（redis十分热衷于造轮子）。看下面的代码，privdata存额外信息，用的很少，至少我们发现。 dictht是具体的哈希表，一个dict对应两张哈希表，这是为了扩容（包括rehashidx也是为了扩容）。dictType存储了哈希表的属性。redis还为dict实现了迭代器（所以说看起来像c++代码）。 哈希表的具体实现是和mc类似的做法，也是使用开链法来解决冲突，不过里面用到了一些小技巧。比如使用dictType存储函数指针，可以动态配置桶里面元素的操作方法。又比如dictht中保存的sizemask取size（桶的数量）-1，用它与key做&amp;操作来代替取余运算，加快速度等等。总的来看，dict里面有两个哈希表，每个哈希表的桶里面存储dictEntry链表，dictEntry存储具体的key和value。 前面说过，一个dict对于两个dictht，是为了扩容（其实还有缩容）。正常的时候，dict只使用dictht[0]，当dict[0]中已有entry的数量与桶的数量达到一定的比例后，就会触发扩容和缩容操作，我们统称为rehash，这时，为dictht[1]申请rehash后的大小的内存，然后把dictht[0]里的数据往dictht[1]里面移动，并用rehashidx记录当前已经移动万的桶的数量，当所有桶都移完后，rehash完成，这时将dictht[1]变成dictht[0], 将原来的dictht[0]变成dictht[1]，并变为null即可。不同于memcached，这里不用开一个后台线程来做，而是就在event loop中完成，并且rehash不是一次性完成，而是分成多次，每次用户操作dict之前，redis移动一个桶的数据，直到rehash完成。这样就把移动分成多个小移动完成，把rehash的时间开销均分到用户每个操作上，这样避免了用户一个请求导致rehash的时候，需要等待很长时间，直到rehash完成才有返回的情况。不过在rehash期间，每个操作都变慢了点，而且用户还不知道redis在他的请求中间添加了移动数据的操作，感觉redis太贱了 :-D 1234567891011121314151617181920212223typedef struct dict &#123; dictType *type; // 哈希表的相关属性void *privdata; // 额外信息 dictht ht[2]; // 两张哈希表，分主和副，用于扩容int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 记录当前数据迁移的位置，在扩容的时候用的int iterators; /* number of iterators currently running */ // 目前存在的迭代器的数量&#125; dict;typedef struct dictht &#123; dictEntry **table; // dictEntry是item，多个item组成hash桶里面的链表，table则是多个链表头指针组成的数组的指针unsigned long size; // 这个就是桶的数量// sizemask取size - 1, 然后一个数据来的时候，通过计算出的hashkey, 让hashkey &amp; sizemask来确定它要放的桶的位置// 当size取2^n的时候，sizemask就是1...111，这样就和hashkey % size有一样的效果，但是使用&amp;会快很多。这就是原因unsigned long sizemask; unsigned long used; // 已经数值的dictEntry数量&#125; dictht;typedef struct dictType &#123; unsigned int (*hashFunction)(const void *key); // hash的方法void *(*keyDup)(void *privdata, const void *key); // key的复制方法void *(*valDup)(void *privdata, const void *obj); // value的复制方法int (*keyCompare)(void *privdata, const void *key1, const void *key2); // key之间的比较void (*keyDestructor)(void *privdata, void *key); // key的析构void (*valDestructor)(void *privdata, void *obj); // value的析构&#125; dictType;typedef struct dictEntry &#123; void *key; union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; struct dictEntry *next;&#125; dictEntry; 有了dict，数据库就好实现了。所有数据读存储在dict中，key存储成dictEntry中的key（string），用void* 指向一个redis object，它可以是5种类型中的任何一种。如下图，结构构造是这样，不过这个图已经过时了，有一些与redis3.0不符合的地方。 五种type的对象，每一个都至少有两种底层实现方式。string有3种：REDIS_ENCODING_RAW, REDIS_ENCIDING_INT, REDIS_ENCODING_EMBSTR， list有：普通双向链表和压缩链表，压缩链表简单的说，就是讲数组改造成链表，连续的空间，然后通过存储字符串的大小信息来模拟链表，相对普通链表来说可以节省空间，不过有副作用，由于是连续的空间，所以改变内存大小的时候，需要重新分配，并且由于保存了字符串的字节大小，所有有可能引起连续更新（具体实现请详细看代码）。set有dict和intset（全是整数的时候使用它来存储）， sorted set有：skiplist和ziplist， hashtable实现有压缩列表和dict和ziplist。skiplist就是跳表，它有接近于红黑树的效率，但是实现起来比红黑树简单很多，所以被采用（奇怪，这里又不造轮子了，难道因为这个轮子有点难？）。 hash table可以使用dict实现，则改dict中，每个dictentry中key保存了key（这是哈希表中的键值对的key），而value则保存了value，它们都是string。 而set中的dict，每个dictentry中key保存了set中具体的一个元素的值，value则为null。图中的zset（有序集合）有误，zset使用skiplist和ziplist实现，首先skiplist很好理解，就把它当做红黑树的替代品就行，和红黑树一样，它也可以排序。怎么用ziplist存储zset呢？首先在zset中，每个set中的元素都有一个分值score，用它来排序。所以在ziplist中，按照分值大小，先存元素，再存它的score，再存下一个元素，然后score。这样连续存储，所以插入或者删除的时候，都需要重新分配内存。所以当元素超过一定数量，或者某个元素的字符数超过一定数量，redis就会选择使用skiplist来实现zset（如果当前使用的是ziplist，会将这个ziplist中的数据取出，存入一个新的skiplist，然后删除改ziplist，这就是底层实现转换，其余类型的redis object也是可以转换的）。 另外，ziplist如何实现hashtable呢？其实也很简单，就是存储一个key，存储一个value，再存储一个key，再存储一个value。还是顺序存储，与zset实现类似，所以当元素超过一定数量，或者某个元素的字符数超过一定数量时，就会转换成hashtable来实现。各种底层实现方式是可以转换的，redis可以根据情况选择最合适的实现方式，这也是这样使用类似面向对象的实现方式的好处。 需要指出的是，使用skiplist来实现zset的时候，其实还用了一个dict，这个dict存储一样的键值对。为什么呢？因为skiplist的查找只是lgn的（可能变成n），而dict可以到O(1)， 所以使用一个dict来加速查找，由于skiplist和dict可以指向同一个redis object，所以不会浪费太多内存。另外使用ziplist实现zset的时候，为什么不用dict来加速查找呢？因为ziplist支持的元素个数很少（个数多时就转换成skiplist了），顺序遍历也很快，所以不用dict了。 这样看来，上面的dict，dictType，dictHt，dictEntry，redis object都是很有考量的，它们配合实现了一个具有面向对象色彩的灵活、高效数据库。不得不说，redis数据库的设计还是很厉害的。 与memcached不同的是，redis的数据库不止一个，默认就有16个，编号0-15。客户可以选择使用哪一个数据库，默认使用0号数据库。 不同的数据库数据不共享，即在不同的数据库中可以存在同样的key，但是在同一个数据库中，key必须是唯一的。 redis也支持expire time的设置，我们看上面的redis object，里面没有保存expire的字段，那redis怎么记录数据的expire time呢？ redis是为每个数据库又增加了一个dict，这个dict叫expire dict，它里面的dict entry里面的key就是数对的key，而value全是数据为64位int的redis object，这个int就是expire time。这样，判断一个key是否过期的时候，去expire dict里面找到它，取出expire time比对当前时间即可。为什么这样做呢？ 因为并不是所有的key都会设置过期时间，所以，对于不设置expire time的key来说，保存一个expire time会浪费空间，而是用expire dict来单独保存的话，可以根据需要灵活使用内存（检测到key过期时，会把它从expire dict中删除）。 redis的expire 机制是怎样的呢？ 与memcahed类似，redis也是惰性删除，即要用到数据时，先检查key是否过期，过期则删除，然后返回错误。单纯的靠惰性删除，上面说过可能会导致内存浪费，所以redis也有补充方案，redis里面有个定时执行的函数，叫servercron，它是维护服务器的函数，在它里面，会对过期数据进行删除，注意不是全删，而是在一定的时间内，对每个数据库的expire dict里面的数据随机选取出来，如果过期，则删除，否则再选，直到规定的时间到。即随机选取过期的数据删除，这个操作的时间分两种，一种较长，一种较短，一般执行短时间的删除，每隔一定的时间，执行一次长时间的删除。这样可以有效的缓解光采用惰性删除而导致的内存浪费问题。 以上就是redis的数据的实现，与memcached不同，redis还支持数据持久化，这个下面介绍。 5.4 redis数据库持久化redis和memcached的最大不同，就是redis支持数据持久化，这也是很多人选择使用redis而不是memcached的最大原因。 redis的持久化，分为两种策略，用户可以配置使用不同的策略。 5.4.1 RDB持久化用户执行save或者bgsave的时候，就会触发RDB持久化操作。RDB持久化操作的核心思想就是把数据库原封不动的保存在文件里。 那如何存储呢？如下图， 首先存储一个REDIS字符串，起到验证的作用，表示是RDB文件，然后保存redis的版本信息，然后是具体的数据库，然后存储结束符EOF，最后用检验和。关键就是databases，看它的名字也知道，它存储了多个数据库，数据库按照编号顺序存储，0号数据库存储完了，才轮到1，然后是2, 一直到最后一个数据库。 ​ 每一个数据库存储方式如下，首先一个1字节的常量SELECTDB，表示切换db了，然后下一个接上数据库的编号，它的长度是可变的，然后接下来就是具体的key-value对的数据了。 1234567891011121314int rdbSaveKeyValuePair(rio *rdb, robj *key, robj *val, long long expiretime, long long now)&#123; /* Save the expire time */if (expiretime != -1) &#123; /* If this key is already expired skip it */if (expiretime &lt; now) return 0; if (rdbSaveType(rdb,REDIS_RDB_OPCODE_EXPIRETIME_MS) == -1) return -1; if (rdbSaveMillisecondTime(rdb,expiretime) == -1) return -1; &#125; /* Save type, key, value */if (rdbSaveObjectType(rdb,val) == -1) return -1; if (rdbSaveStringObject(rdb,key) == -1) return -1; if (rdbSaveObject(rdb,val) == -1) return -1; return 1;&#125; 由上面的代码也可以看出，存储的时候，先检查expire time，如果已经过期，不存就行了，否则，则将expire time存下来，注意，及时是存储expire time，也是先存储它的类型为REDIS_RDB_OPCODE_EXPIRETIME_MS，然后再存储具体过期时间。接下来存储真正的key-value对，首先存储value的类型，然后存储key（它按照字符串存储），然后存储value，如下图。 在rdbsaveobject中，会根据val的不同类型，按照不同的方式存储，不过从根本上来看，最终都是转换成字符串存储，比如val是一个linklist，那么先存储整个list的字节数，然后遍历这个list，把数据取出来，依次按照string写入文件。对于hash table，也是先计算字节数，然后依次取出hash table中的dictEntry，按照string的方式存储它的key和value，然后存储下一个dictEntry。 总之，RDB的存储方式，对一个key-value对，会先存储expire time（如果有的话），然后是value的类型，然后存储key（字符串方式），然后根据value的类型和底层实现方式，将value转换成字符串存储。这里面为了实现数据压缩，以及能够根据文件恢复数据，redis使用了很多编码的技巧，有些我也没太看懂，不过关键还是要理解思想，不要在意这些细节。 保存了RDB文件，当redis再启动的时候，就根据RDB文件来恢复数据库。由于以及在RDB文件中保存了数据库的号码，以及它包含的key-value对，以及每个key-value对中value的具体类型，实现方式，和数据，redis只要顺序读取文件，然后恢复object即可。由于保存了expire time，发现当前的时间已经比expire time大了，即数据已经超时了，则不恢复这个key-value对即可。 保存RDB文件是一个很巨大的工程，所以redis还提供后台保存的机制。即执行bgsave的时候，redis fork出一个子进程，让子进程来执行保存的工作，而父进程继续提供redis正常的数据库服务。由于子进程复制了父进程的地址空间，即子进程拥有父进程fork时的数据库，子进程执行save的操作，把它从父进程那儿继承来的数据库写入一个temp文件即可。在子进程复制期间，redis会记录数据库的修改次数（dirty）。当子进程完成时，发送给父进程SIGUSR1信号，父进程捕捉到这个信号，就知道子进程完成了复制，然后父进程将子进程保存的temp文件改名为真正的rdb文件（即真正保存成功了才改成目标文件，这才是保险的做法）。然后记录下这一次save的结束时间。 ​ 这里有一个问题，在子进程保存期间，父进程的数据库已经被修改了，而父进程只是记录了修改的次数（dirty），被没有进行修正操作。似乎使得RDB保存的不是实时的数据库，有点不太高大上的样子。 不过后面要介绍的AOF持久化，就解决了这个问题。 除了客户执行sava或者bgsave命令，还可以配置RDB保存条件。即在配置文件中配置，在t时间内，数据库被修改了dirty次，则进行后台保存。redis在serve cron的时候，会根据dirty数目和上次保存的时间，来判断是否符合条件，符合条件的话，就进行bg save，注意，任意时刻只能有一个子进程来进行后台保存，因为保存是个很费io的操作，多个进程大量io效率不行，而且不好管理。 5.4.2 AOF持久化 首先想一个问题，保存数据库一定需要像RDB那样把数据库里面的所有数据保存下来么？有没有别的方法？ RDB保存的只是最终的数据库，它是一个结果。结果是怎么来的？是通过用户的各个命令建立起来的，所以可以不保存结果，而只保存建立这个结果的命令。 redis的AOF就是这个思想，它不同RDB保存db的数据，它保存的是一条一条建立数据库的命令。 我们首先来看AOF文件的格式，它里面保存的是一条一条的命令，首先存储命令长度，然后存储命令，具体的分隔符什么的可以自己深入研究，这都不是重点，反正知道AOF文件存储的是redis客户端执行的命令即可。 redis server中有一个sds aof_buf, 如果aof持久化打开的话，每个修改数据库的命令都会存入这个aof_buf（保存的是aof文件中命令格式的字符串），然后event loop没循环一次，在server cron中调用flushaofbuf，把aof_buf中的命令写入aof文件（其实是write，真正写入的是内核缓冲区），再清空aof_buf，进入下一次loop。这样所有的数据库的变化，都可以通过aof文件中的命令来还原，达到了保存数据库的效果。 需要注意的是，flushaofbuf中调用的write，它只是把数据写入了内核缓冲区，真正写入文件时内核自己决定的，可能需要延后一段时间。 不过redis支持配置，可以配置每次写入后sync，则在redis里面调用sync，将内核中的数据写入文件，这不过这要耗费一次系统调用，耗费时间而已。还可以配置策略为1秒钟sync一次，则redis会开启一个后台线程（所以说redis不是单线程，只是单eventloop而已），这个后台线程会每一秒调用一次sync。这里要问了，RDB的时候为什么没有考虑sync的事情呢？因为RDB是一次性存储的，不像AOF这样多次存储，RDB的时候调用一次sync也没什么影响，而且使用bg save的时候，子进程会自己退出（exit），这时候exit函数内会冲刷缓冲区，自动就写入了文件中。 再来看，如果不想使用aof_buf保存每次的修改命令，也可以使用aof持久化。redis提供aof_rewrite，即根据现有的数据库生成命令，然后把命令写入aof文件中。很奇特吧？对，就是这么厉害。进行aof_rewrite的时候，redis变量每个数据库，然后根据key-value对中value的具体类型，生成不同的命令，比如是list，则它生成一个保存list的命令，这个命令里包含了保存该list所需要的的数据，如果这个list数据过长，还会分成多条命令，先创建这个list，然后往list里面添加元素，总之，就是根据数据反向生成保存数据的命令。然后将这些命令存储aof文件，这样不就和aof append达到同样的效果了么？ aof格式也支持后台模式。执行aof_bgrewrite的时候，也是fork一个子进程，然后让子进程进行aof_rewrite，把它复制的数据库写入一个临时文件，然后写完后用新号通知父进程。父进程判断子进程的退出信息是否正确，然后将临时文件更名成最终的aof文件。好了，问题来了。在子进程持久化期间，可能父进程的数据库有更新，怎么把这个更新通知子进程呢？难道要用进程间通信么？是不是有点麻烦呢？你猜redis怎么做的？它根本不通知子进程。什么，不通知？那更新怎么办？ 在子进程执行aof_bgrewrite期间，父进程会保存所有对数据库有更改的操作的命令（增，删除，改等），把他们保存在aof_rewrite_buf_blocks中，这是一个链表，每个block都可以保存命令，存不下时，新申请block，然后放入链表后面即可，当子进程通知完成保存后，父进程将aof_rewrite_buf_blocks的命令append 进aof文件就可以了。多么优美的设计，想一想自己当初还考虑用进程间通信，别人直接用最简单的方法就完美的解决了问题，有句话说得真对，越优秀的设计越趋于简单，而复杂的东西往往都是靠不住的。 至于aof文件的载入，也就是一条一条的执行aof文件里面的命令而已。不过考虑到这些命令就是客户端发送给redis的命令，所以redis干脆生成了一个假的客户端，它没有和redis建立网络连接，而是直接执行命令即可。首先搞清楚，这里的假的客户端，并不是真正的客户端，而是存储在redis里面的客户端的信息，里面有写和读的缓冲区，它是存在于redis服务器中的。所以，如下图，直接读入aof的命令，放入客户端的读缓冲区中，然后执行这个客户端的命令即可。这样就完成了aof文件的载入。 123456789101112// 创建伪客户端fakeClient = createFakeClient();while(命令不为空) &#123; // 获取一条命令的参数信息 argc， argv ... // 执行 fakeClient-&gt;argc = argc; fakeClient-&gt;argv = argv; cmd-&gt;proc(fakeClient);&#125; 整个aof持久化的设计，个人认为相当精彩。其中有很多地方，值得膜拜。 5.5 redis的事务 redis另一个比memcached强大的地方，是它支持简单的事务。事务简单说就是把几个命令合并，一次性执行全部命令。对于关系型数据库来说，事务还有回滚机制，即事务命令要么全部执行成功，只要有一条失败就回滚，回到事务执行前的状态。redis不支持回滚，它的事务只保证命令依次被执行，即使中间一条命令出错也会继续往下执行，所以说它只支持简单的事务。 首先看redis事务的执行过程。首先执行multi命令，表示开始事务，然后输入需要执行的命令，最后输入exec执行事务。 redis服务器收到multi命令后，会将对应的client的状态设置为REDIS_MULTI，表示client处于事务阶段，并在client的multiState结构体里面保持事务的命令具体信息（当然首先也会检查命令是否能否识别，错误的命令不会保存），即命令的个数和具体的各个命令，当收到exec命令后，redis会顺序执行multiState里面保存的命令，然后保存每个命令的返回值，当有命令发生错误的时候，redis不会停止事务，而是保存错误信息，然后继续往下执行，当所有的命令都执行完后，将所有命令的返回值一起返回给客户。redis为什么不支持回滚呢？网上看到的解释出现问题是由于客户程序的问题，所以没必要服务器回滚，同时，不支持回滚，redis服务器的运行高效很多。在我看来，redis的事务不是传统关系型数据库的事务，要求CIAD那么非常严格，或者说redis的事务都不是事务，只是提供了一种方式，使得客户端可以一次性执行多条命令而已，就把事务当做普通命令就行了，支持回滚也就没必要了。 ​ 我们知道redis是单event loop的，在真正执行一个事物的时候（即redis收到exec命令后），事物的执行过程是不会被打断的，所有命令都会在一个event loop中执行完。但是在用户逐个输入事务的命令的时候，这期间，可能已经有别的客户修改了事务里面用到的数据，这就可能产生问题。所以redis还提供了watch命令，用户可以在输入multi之前，执行watch命令，指定需要观察的数据，这样如果在exec之前，有其他的客户端修改了这些被watch的数据，则exec的时候，执行到处理被修改的数据的命令的时候，会执行失败，提示数据已经dirty。 这是如何是实现的呢？ 原来在每一个redisDb中还有一个dict watched_keys，watched_kesy中dictentry的key是被watch的数据库的key，而value则是一个list，里面存储的是watch它的client。同时，每个client也有一个watched_keys，里面保存的是这个client当前watch的key。在执行watch的时候，redis在对应的数据库的watched_keys中找到这个key（如果没有，则新建一个dictentry），然后在它的客户列表中加入这个client，同时，往这个client的watched_keys中加入这个key。当有客户执行一个命令修改数据的时候，redis首先在watched_keys中找这个key，如果发现有它，证明有client在watch它，则遍历所有watch它的client，将这些client设置为REDIS_DIRTY_CAS，表面有watch的key被dirty了。当客户执行的事务的时候，首先会检查是否被设置了REDIS_DIRTY_CAS，如果是，则表明数据dirty了，事务无法执行，会立即返回错误，只有client没有被设置REDIS_DIRTY_CAS的时候才能够执行事务。 需要指出的是，执行exec后，该client的所有watch的key都会被清除，同时db中该key的client列表也会清除该client，即执行exec后，该client不再watch任何key（即使exec没有执行成功也是一样）。所以说redis的事务是简单的事务，算不上真正的事务。 以上就是redis的事务，感觉实现很简单，实际用处也不是太大。 5.6 redis的发布订阅频道 redis支持频道，即加入一个频道的用户相当于加入了一个群，客户往频道里面发的信息，频道里的所有client都能收到。 实现也很简单，也watch_keys实现差不多，redis server中保存了一个pubsub_channels的dict，里面的key是频道的名称（显然要唯一了），value则是一个链表，保存加入了该频道的client。同时，每个client都有一个pubsub_channels，保存了自己关注的频道。当用用户往频道发消息的时候，首先在server中的pubsub_channels找到改频道，然后遍历client，给他们发消息。而订阅，取消订阅频道不够都是操作pubsub_channels而已，很好理解。 同时，redis还支持模式频道。即通过正则匹配频道，如有模式频道p, 1, 则向普通频道p1发送消息时，会匹配p，1，除了往普通频道发消息外，还会往p，1模式频道中的client发消息。注意，这里是用发布命令里面的普通频道来匹配已有的模式频道，而不是在发布命令里制定模式频道，然后匹配redis里面保存的频道。实现方式也很简单，在redis server里面有个pubsub_patterns的list（这里为什么不用dict？因为pubsub_patterns的个数一般较少，不需要使用dict，简单的list就好了），它里面存储的是pubsubPattern结构体，里面是模式和client信息，如下所示，一个模式，一个client，所以如果有多个clint监听一个pubsub_patterns的话，在list面会有多个pubsubPattern，保存client和pubsub_patterns的对应关系。 同时，在client里面，也有一个pubsub_patterns list，不过里面存储的就是它监听的pubsub_patterns的列表（就是sds），而不是pubsubPattern结构体。 1234typedef struct pubsubPattern &#123; redisClient *client; // 监听的client robj *pattern; // 模式&#125; pubsubPattern; 当用户往一个频道发送消息的时候，首先会在redis server中的pubsub_channels里面查找该频道，然后往它的客户列表发送消息。然后在redis server里面的pubsub_patterns里面查找匹配的模式，然后往client里面发送消息。 这里并没有去除重复的客户，在pubsub_channels可能已经给某一个client发过message了，然后在pubsub_patterns中可能还会给用户再发一次（甚至更多次）。 估计redis认为这是客户程序自己的问题，所以不处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/* Publish a message */int pubsubPublishMessage(robj *channel, robj *message) &#123; int receivers = 0; dictEntry *de; listNode *ln; listIter li; /* Send to clients listening for that channel */ de = dictFind(server.pubsub_channels,channel); if (de) &#123; list *list = dictGetVal(de); listNode *ln; listIter li; listRewind(list,&amp;li); while ((ln = listNext(&amp;li)) != NULL) &#123; redisClient *c = ln-&gt;value; addReply(c,shared.mbulkhdr[3]); addReply(c,shared.messagebulk); addReplyBulk(c,channel); addReplyBulk(c,message); receivers++; &#125; &#125; /* Send to clients listening to matching channels */if (listLength(server.pubsub_patterns)) &#123; listRewind(server.pubsub_patterns,&amp;li); channel = getDecodedObject(channel); while ((ln = listNext(&amp;li)) != NULL) &#123; pubsubPattern *pat = ln-&gt;value; if (stringmatchlen((char*)pat-&gt;pattern-&gt;ptr, sdslen(pat-&gt;pattern-&gt;ptr), (char*)channel-&gt;ptr, sdslen(channel-&gt;ptr),0)) &#123; addReply(pat-&gt;client,shared.mbulkhdr[4]); addReply(pat-&gt;client,shared.pmessagebulk); addReplyBulk(pat-&gt;client,pat-&gt;pattern); addReplyBulk(pat-&gt;client,channel); addReplyBulk(pat-&gt;client,message); receivers++; &#125; &#125; decrRefCount(channel); &#125; return receivers;&#125; 6. 总结总的来看，redis比memcached的功能多很多，实现也更复杂。 不过memcached更专注于保存key-value数据（这已经能满足大多数使用场景了），而redis提供更丰富的数据结构及其他的一些功能。不能说redis比memcached好，不过从源码阅读的角度来看，redis的价值或许更大一点。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://vincentruan.github.io/tags/redis/"},{"name":"memcached","slug":"memcached","permalink":"https://vincentruan.github.io/tags/memcached/"}]},{"title":"[转载]从零开始：史上最详尽V2Ray搭建图文教程","slug":"转载-从零开始：史上最详尽V2Ray搭建图文教程","date":"2019-12-22T09:05:18.000Z","updated":"2019-12-22T09:16:39.439Z","comments":true,"path":"2019/12/22/转载-从零开始：史上最详尽V2Ray搭建图文教程/","link":"","permalink":"https://vincentruan.github.io/2019/12/22/转载-从零开始：史上最详尽V2Ray搭建图文教程/","excerpt":"","text":"本文转载自从零开始：史上最详尽V2Ray搭建图文教程，根据实际服务器配置做部分修改。 一、服务端安装以下所有操作都是使用root用户（普通用户自行sudo）进行操作的，服务器centos7。 1.安装wget 如提示没有安装wget，在登录完成的窗口输入下面命令并回车进行wget安装： 1yum -y install wget 2.下载脚本 安装完wget之后就可以进行下载安装v2ray的脚本了，输入如下命令并回车： 1wget https://install.direct/go.sh 3.安装unzip 因为centos不支持apt-get，我们需要安装unzip，详见官方说明： 1yum install -y zip unzip 4.执行安装 输入下面的命令并回车执行安装 123456789101112131415161718192021222324252627[michael@centos74 v2ray]$ bash go.sh Installing V2Ray v3.14 on x86_64Downloading V2Ray. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 608 0 608 0 0 2229 0 --:--:-- --:--:-- --:--:-- 2235100 8482k 100 8482k 0 0 2501k 0 0:00:03 0:00:03 --:--:-- 2813kExtracting V2Ray package to /tmp/v2ray.Archive: /tmp/v2ray/v2ray.zip creating: /tmp/v2ray/v2ray-v3.14-linux-64/ inflating: /tmp/v2ray/v2ray-v3.14-linux-64/geoip.dat inflating: /tmp/v2ray/v2ray-v3.14-linux-64/geosite.dat inflating: /tmp/v2ray/v2ray-v3.14-linux-64/readme.md creating: /tmp/v2ray/v2ray-v3.14-linux-64/systemd/ inflating: /tmp/v2ray/v2ray-v3.14-linux-64/systemd/v2ray.service creating: /tmp/v2ray/v2ray-v3.14-linux-64/systemv/ inflating: /tmp/v2ray/v2ray-v3.14-linux-64/systemv/v2ray inflating: /tmp/v2ray/v2ray-v3.14-linux-64/v2ctl extracting: /tmp/v2ray/v2ray-v3.14-linux-64/v2ctl.sig inflating: /tmp/v2ray/v2ray-v3.14-linux-64/v2ray extracting: /tmp/v2ray/v2ray-v3.14-linux-64/v2ray.sig inflating: /tmp/v2ray/v2ray-v3.14-linux-64/vpoint_socks_vmess.json inflating: /tmp/v2ray/v2ray-v3.14-linux-64/vpoint_vmess_freedom.json PORT:13437UUID:f500ecf5-e135-49c6-9ce2-78eb490d0aa9Created symlink from /etc/systemd/system/multi-user.target.wants/v2ray.service to /etc/systemd/system/v2ray.service.V2Ray v3.14 is installed. 5.相关命令 在首次安装完成之后，V2Ray不会自动启动，需要手动运行上述启动命令。而在已经运行V2Ray的VPS上再次执行安装脚本，安装脚本会自动停止V2Ray 进程，升级V2Ray程序，然后自动运行V2Ray。在升级过程中，配置文件不会被修改。 1234567891011## 启动systemctl start v2ray## 停止systemctl stop v2ray## 重启systemctl restart v2ray## 开机自启systemctl enable v2ray 关于软件更新：更新 V2Ray 的方法是再次执行安装脚本！再次执行安装脚本！再次执行安装脚本！ 6.配置 如果你按照上面的命令执行安装完成之后，服务端其实是不需要再进行任何配置的，配置文件位于/etc/v2ray/config.json，使用cat /etc/v2ray/config.json查看配置信息。接下来进行客户端配置就行了。 说明： 配置文件中的id、端口、alterId需要和客户端的配置保持一致； 服务端使用脚本安装成功之后默认就是vmess协议； 配置完成之后重启v2ray。 9.防火墙开放端口 有的vps端口默认不开放，可能导致连接不成功，如果有这种情况，详细配置，见CentOs开放端口的方法—二、firewalld。部分服务器的防火墙配置只能在服务提供商的控制台操作，请注意。 12345## 查看已开放端口firewall-cmd --zone=public --list-ports## 添加开放端口firewall-cmd --zone=public --add-port=80/tcp --permanent 二、Windows 客户端1.下载 目前不支持水果系列，水果机只能自行走野路子解决。 1)下载【v2ray-windows-64.zip Github Release】;2)下载【v2rayN-v2rayN.exe-Github Release】； 对v2ray-windows-64.zip进行解压，然后将下载的V2RayN.exe复制到解压后的目录，即两个下载好的文件需要在同一目录。 2.配置 运行V2RayN.exe，然后进行配置，下图中的配置信息，需要和你VPS搭建的时候的配置信息对应，VPS的v2ray配置信息位于/etc/v2ray/config.json文件里。 如果采用上面的默认方式安装，服务端配置是协议vmess，则配置如下： 三、测试打开浏览器，访问www.google.com 四、进阶现在你已经学会使用v2ray了，为了更好的上网效果，建议继续了解一下下面文章： centos7基于nginx搭建v2ray服务端配置vmess+tls+websocket完全手册；【推荐】 使用Google BBR PLUS加速你的VPS网络； 如何以mkcp方式部署v2ray； 五、相关问题 使用v2ray访问谷歌提示异常流量； 启用cloudflare cdn之后v2ray报403错误；","categories":[],"tags":[{"name":"v2ray","slug":"v2ray","permalink":"https://vincentruan.github.io/tags/v2ray/"}]},{"title":"详解分布式协调服务 ZooKeeper","slug":"详解分布式协调服务-ZooKeeper","date":"2018-10-07T04:14:40.000Z","updated":"2018-10-07T05:24:39.622Z","comments":true,"path":"2018/10/07/详解分布式协调服务-ZooKeeper/","link":"","permalink":"https://vincentruan.github.io/2018/10/07/详解分布式协调服务-ZooKeeper/","excerpt":"","text":"作者 | Draveness 本文作者 Draveness，文章转载自 https://draveness.me/zookeeper-chubby， 对其内容进行过编辑。 这篇文章主要会介绍 Zookeeper 的实现原理以及常见的应用 在 2006 年，Google 发表了一篇名为 The Chubby lock service for loosely-coupled distributed systems 的论文，其中描述了一个分布式锁服务 Chubby 的设计理念和实现原理；作为 Google 内部的一个基础服务，虽然 Chubby 与 GFS、Bigtable 和 MapReduce 相比并没有那么大的名气，不过它在 Google 内部也是非常重要的基础设施。 相比于名不见经传的 Chubby，作者相信 Zookeeper 更被广大开发者所熟知，作为非常出名的分布式协调服务，Zookeeper 有非常多的应用，包括发布订阅、命名服务、分数是协调和分布式锁，这篇文章主要会介绍 Zookeeper 的实现原理以及常见的应用，但是在具体介绍 Zookeeper 的功能和原理之前，我们会简单介绍一下分布式锁服务 Chubby 以及它与 Zookeeper 之间的异同。 Chubby作为分布式锁服务，Chubby 的目的就是允许多个客户端对它们的行为进行同步，同时也能够解决客户端的环境相关信息的分发和粗粒度的同步问题，GFS 和 Bigtable 都使用了 Chubby 以解决主节点的选举等问题。在网络上你很难找到关于 Chubby 的相关资料，我们只能从 The Chubby lock service for loosely-coupled distributed systems 一文中窥见它的一些设计思路、技术架构等信息。 虽然 Chubby 和 Zookeeper 有着比较相似的功能，但是它们的设计理念却非常不同，Chubby 在论文的摘要中写道： We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. 从论文的摘要中我们可以看出 Chubby 首先被定义成一个 分布式的锁服务，它能够为分布式系统提供 松耦合、粗粒度 的分布式锁功能，然而我们并不能依赖于它来做一些重量的数据存储。 Chubby 在设计时做了两个重要的设计决定，一是提供完整、独立的分布式锁服务而非一个用于共识的库或者服务，另一个是选择提供小文件的的读写功能，使得主节点能够方便地发布自己的状态信息。 系统架构Chubby 总共由两部分组成，一部分是用于提供数据的读写接口并管理相关的配置数据的服务端，另一部分就是客户端使用的 SDK，为了提高系统的稳定性，每一个 Chubby 单元都由一组服务器组成，它会使用 共识算法 从集群中选举出主节点。 在一个 Chubby Cell 中，只有 主节点会对外提供读写服务，其他的节点其实都是当前节点的副本（Replica），它们只是维护一个数据的拷贝并会在主节点更新时对它们持有的数据库进行更新；客户端通过向副本发送请求获取主节点的位置，一旦它获取到了主节点的位置，就会向所有的读写请求发送给主节点，直到其不再响应为止。写请求都会通过一致性协议传播到所有的副本中，当集群中的多数节点都同步了请求时就会认为当前的写入已经被确认。 当主节点宕机时，副本会在其租约到期时重新进行选举，副本节点如果在宕机几小时还没有回复，那么系统就会从资源池中选择一个新的节点并在该节点上启动 Chubby 服务并更新 DNS 表。 主节点会不停地轮训 DNS 表获取集群中最新的配置，每次 DNS 表更新时，主节点都会将新的配置下发给 Chubby 集群中其他的副本节点。 Zookeeper很多人都会说 Zookeeper 是 Chubby 的一个开源实现，这其实是有问题的，它们两者只不过都提供了具有层级结构的命名空间： Chubby 和 Zookeeper 从最根本的设计理念上就有着非常明显的不同，在上文中我们已经提到了 Chubby 被设计成一个分布式的锁服务，它能够为分布式系统提供松耦合、粗粒度的分布式锁功能，然而我们并不能依赖于它来做一些重量的数据存储，而 Zookeeper 的论文在摘要中介绍到，它是一个能够为分布式系统提供协调功能的服务： In this paper, we describe ZooKeeper, a service for co- ordinating processes of distributed applications. Zookeeper 的目的是为客户端构建复杂的协调功能提供简单、高效的核心 API，相比于 Chubby 对外提供已经封装好的更上层的功能，Zookeeper 提供了更抽象的接口以便于客户端自行实现想要完成的功能。 Chubby 直接为用户提供封装好的锁和解锁的功能，内部完成了锁的实现，只是将 API 直接暴露给用户，而 Zookeeper 却需要用户自己实现分布式锁；总的来说，使用 Zookeeper 往往需要客户端做更多的事情，但是也享有更多的自由。 技术架构与 Chubby 集群中，多个节点只有一个能够对外提供服务不同，Zookeeper 集群中所有的节点都可以对外提供服务，但是集群中的节点也分为主从两种节点，所有的节点都能处理来自客户端的读请求，但是只有主节点才能处理写入操作： 这里所说的 Zookeeper 集群主从节点实际上分别是 Leader 和 Follower 节点。 客户端使用 Zookeeper 时会连接到集群中的任意节点，所有的节点都能够直接对外提供读操作，但是写操作都会被从节点路由到主节点，由主节点进行处理。 Zookeeper 在设计上提供了以下的两个基本的顺序保证，线性写和先进先出的客户端顺序： 其中线性写是指所有更新 Zookeeper 状态的请求都应该按照既定的顺序串行执行；而先进先出的客户端顺序是指，所有客户端发出的请求会按照发出的顺序执行。 Zab 协议在我们简单介绍 Zookeeper 的技术架构之后，这一节将谈及 Zookeeper 中的 Zab 协议，Zookeeper 的 Zab 协议是为了解决分布式一致性而设计出的一种协议，它的全称是 Zookeeper 原子广播协议，它能够在发生崩溃时快速恢复服务，达到高可用性。 如上一节提到的，客户端在使用 Zookeeper 服务时会随机连接到集群中的一个节点，所有的读请求都会由当前节点处理，而写请求会被路由给主节点并由主节点向其他节点广播事务，与 2PC 非常相似，如果在所有的节点中超过一半都返回成功，那么当前写请求就会被提交。 当主节点崩溃时，其他的 Replica 节点会进入崩溃恢复模式并重新进行选举，Zab 协议必须确保提交已经被 Leader 提交的事务提案，同时舍弃被跳过的提案，这也就是说当前集群中最新 ZXID 最大的服务器会被选举成为 Leader 节点；但是在正式对外提供服务之前，新的 Leader 也需要先与 Follower 中的数据进行同步，确保所有节点拥有完全相同的提案列表。 在上面提到 ZXID 其实就是 Zab 协议中设计的事务编号，它是一个 64 位的整数，其中最低的 32 位是一个计数器，每当客户端修改 Zookeeper 集群状态时，Leader 都会以当前 ZXID 值作为提案的编号创建一个新的事务，在这之后会将当前计数器加一；ZXID 中高的 32 位表示当前 Leader 的任期，每当发生崩溃进入恢复模式，集群的 Leader 重新选举之后都会将 epoch 加一。 Zab 和 PaxosZab 和 Paxos 协议在实现上其实有非常多的相似点，例如： 主节点会向所有的从节点发出提案； 主节点在接收到一组从节点中 50% 以上节点的确认后，才会认为当前提案被提交了； Zab 协议中的每一个提案都包含一个 epoch 值，与 Paxos 中的 Ballot 非常相似； 因为它们有一些相同的特点，所以有的观点会认为 Zab 是 Paxos 的一个简化版本，但是 Zab 和 Paxos 在设计理念上就有着比较大的不同，两者的主要区别就在于 Zab 主要是为构建高可用的主备系统设计的，而 Paxos 能够帮助工程师搭建具有一致性的状态机系统。 作为一个一致性状态机系统，它能够保证集群中任意一个状态机副本都按照客户端的请求执行了相同顺序的请求，即使来自客户端请求是异步的并且不同客户端的接收同一个请求的顺序不同，集群中的这些副本就是会使用 Paxos 或者它的变种对提案达成一致；在集群运行的过程中，如果主节点出现了错误导致宕机，其他的节点会重新开始进行选举并处理未提交的请求。 但是在类似 Zookeeper 的高可用主备系统中，所有的副本都需要对增量的状态更新顺序达成一致，这些状态更新的变量都是由主节点创建并发送给其他的从节点的，每一个从节点都会严格按照顺序逐一的执行主节点生成的状态更新请求，如果 Zookeeper 集群中的主节点发生了宕机，新的主节点也必须严格按照顺序对请求进行恢复。 总的来说，使用状态更新节点数据的主备系统相比根据客户端请求改变状态的状态机系统对于请求的执行顺序有着更严格的要求。 实现原理这一节会简单介绍 Zookeeper 的一些实现原理，重点会介绍以下几个部分的内容：文件系统、临时 / 持久节点和通知的实现原理。 文件系统了解或者使用 Zookeeper 或者其他分布式协调服务的读者对于使用类似文件系统的方式比较熟悉，与 Unix 中的文件系统份上相似的是，Zookeeper 中也使用文件系统组织系统中存储的资源。 Zookeeper 中其实并没有文件和文件夹的概念，它只有一个 Znode 的概念，它既能作为容器存储数据，也可以持有其他的 Znode 形成父子关系。 Znode 其实有 PERSISTENT、PERSISTENT_SEQUENTIAL、EPHEMERAL 和 EPHEMERAL_SEQUENTIAL 四种类型，它们是临时与持久、顺序与非顺序两个不同的方向组合成的四种类型。 临时节点是客户端在连接 Zookeeper 时才会保持存在的节点，一旦客户端和服务端之间的连接中断，当前连接持有的所有节点都会被删除，而持久的节点不会随着会话连接的中断而删除，它们需要被客户端主动删除；Zookeeper 中另一种节点的特性就是顺序和非顺序，如果我们使用 Zookeeper 创建了顺序的节点，那么所有节点就会在名字的末尾附加一个序列号，序列号是一个由父节点维护的单调递增计数器。 通知常见的通知机制往往都有两种，一种是客户端使用『拉』的方式从服务端获取最新的状态，这种方式获取的状态很有可能都是过期的，需要客户端不断地通过轮训的方式获取服务端最新的状态，另一种方式就是在客户端订阅对应节点后由服务端向所有订阅者推送该节点的变化，相比于客户端主动获取数据的方式，服务端主动推送更能够保证客户端数据的实时性。 作为分布式协调工具的 Zookeeper 就实现了这种服务端主动推送请求的机制，也就是 Watch，当客户端使用 getData 等接口获取 Znode 状态时传入了一个用于处理节点变更的回调，那么服务端就会主动向客户端推送节点的变更： 1public byte[] getData(final String path, Watcher watcher, Stat stat) 从这个方法中传入的 Watcher 对象实现了相应的 process 方法，每次对应节点出现了状态的改变，WatchManager 都会通过以下的方式调用传入 Watcher 的方法： 1234567891011Set&lt;Watcher&gt; triggerWatch(String path, EventType type, Set&lt;Watcher&gt; supress) &#123; WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path); Set&lt;Watcher&gt; watchers; synchronized (this) &#123; watchers = watchTable.remove(path); &#125; for (Watcher w : watchers) &#123; w.process(e); &#125; return watchers;&#125; Zookeeper 中的所有数据其实都是由一个名为 DataTree 的数据结构管理的，所有的读写数据的请求最终都会改变这颗树的内容，在发出读请求时可能会传入 Watcher 注册一个回调函数，而写请求就可能会触发相应的回调，由 WatchManager 通知客户端数据的变化。 通知机制的实现其实还是比较简单的，通过读请求设置 Watcher 监听事件，写请求在触发事件时就能将通知发送给指定的客户端。 会话在 Zookeeper 中一个非常重要的概念就是会话，客户端与服务器之间的任何操作都与 Zookeeper 中会话的概念有关，比如我们再上一节中提到的临时节点生命周期以及通知的机制等等，它们都是基于会话来实现的。 每当客户端与服务端建立连接时，其实创建了一个新的会话，在每一个会话的生命周期中，Zookeeper 会在不同的会话状态之间进行切换，比如说：CONNECTING、CONNECTED、RECONNECTING、RECONNECTED 和 CLOSE 等。 作为 Zookeeper 中最重要的概念之一，每一个 Session 都包含四个基本属性，会话的唯一 ID、会话超时时间、下次会话的超时时间点和表示会话是否被关闭的标记。 SessionTracker 是 Zookeeper 中的会话管理器，它负责所有会话的创建、管理以及清理工作，但是它本身只是一个 Java 的接口，定义了一系列用于管理会话的相关接口： 1234567891011121314151617181920public interface SessionTracker &#123; public static interface Session &#123; long getSessionId(); int getTimeout(); boolean isClosing(); &#125; public static interface SessionExpirer &#123; void expire(Session session); long getServerId(); &#125; long createSession(int sessionTimeout); boolean trackSession(long id, int to); boolean commitSession(long id, int to); boolean touchSession(long sessionId, int sessionTimeout); void setSessionClosing(long sessionId); void shutdown(); void removeSession(long sessionId);&#125; 与其他的长连接一样，Zookeeper 中的会话也需要客户端与服务端之间进行心跳检测，客户端会在超时时间内向服务端发送心跳请求来保证会话不会被服务端关闭，一旦服务端检测到某一个会话长时间没有收到心跳包就会中断当前会话释放服务器上的资源。 应用作为分布式协调服务，Zookeeper 能够为集群提供分布式一致性的保证，我们可以通过 Zookeeper 提供的最基本的 API 组合成更高级的功能： 12345678public class Zookeeper &#123; public String create(final String path, byte data[], List&lt;ACL&gt; acl, CreateMode createMode) public void delete(final String path, int version) throws InterruptedException, KeeperException public Stat exists(final String path, Watcher watcher) throws KeeperException, InterruptedException public byte[] getData(final String path, Watcher watcher, Stat stat) throws KeeperException, InterruptedException public Stat setData(final String path, byte data[], int version) throws KeeperException, InterruptedException public void sync(final String path, VoidCallback cb, Object ctx)&#125; 在这一节中，我们将介绍如何在生产环境中使用 Zookeeper 实现发布订阅、命名服务、分布式协调以及分布式锁等功能。 发布订阅通过 Zookeeper 进行数据的发布与订阅其实可以说是它提供的最基本功能，它能够允许多个客户端同时订阅某一个节点的变更并在变更发生时执行我们预先设置好的回调函数，在运行时改变服务的配置和行为： 123456789ZooKeeper zk = new ZooKeeper(\"localhost\", 3000, null);zk.getData(\"/config\", new Watcher() &#123; public void process(WatchedEvent watchedEvent) &#123; System.out.println(watchedEvent.toString()); &#125;&#125;, null);zk.setData(\"/config\", \"draven\".getBytes(), 0);// WatchedEvent state:SyncConnected type:NodeDataChanged path:/config 发布与订阅是 Zookeeper 提供的一个最基本的功能，它的使用非常的简单，我们可以在 getData 中传入实现 process 方法的 Watcher 对象，在每次改变节点的状态时，process 方法都会被调用，在这个方法中就可以对变更进行响应动态修改一些行为。 通过 Zookeeper 这个中枢，每一个客户端对节点状态的改变都能够推送给节点的订阅者，在发布订阅模型中，Zookeeper 的每一个节点都可以被理解成一个主题，每一个客户端都可以向这个主题推送详细，同时也可以订阅这个主题中的消息；只是 Zookeeper 引入了文件系统的父子层级的概念将发布订阅功能实现得更加复杂。 1234567public static enum EventType &#123; None(-1), NodeCreated(1), NodeDeleted(2), NodeDataChanged(3), NodeChildrenChanged(4);&#125; 如果我们订阅了一个节点的变更信息，那么该节点的子节点出现数量变更时就会调用 process 方法通知观察者，这也意味着更复杂的实现，同时和专门做发布订阅的中间件相比也没有性能优势，在海量推送的应用场景下，消息队列更能胜任，而 Zookeeper 更适合做一些类似服务配置的动态下发的工作。 命名服务除了实现服务配置数据的发布与订阅功能，Zookeeper 还能帮助分布式系统实现命名服务，在每一个分布式系统中，客户端应用都有根据指定名字获取资源、服务器地址的需求，在这时就要求整个集群中的全部服务有着唯一的名字。 在大型分布式系统中，有两件事情非常常见，一是不同服务之间的可能拥有相同的名字，另一个是同一个服务可能会在集群中部署很多的节点，Zookeeper 就可以通过文件系统和顺序节点解决这两个问题。 在上图中，我们创建了两个命名空间，/infrastructure 和 /business 分别代表架构和业务部门，两个部门中都拥有名为 metrics 的服务，而业务部门的 metrics 服务也部署了两个节点，在这里使用了命名空间和顺序节点解决唯一标志符的问题。 1234567ZooKeeper zk = new ZooKeeper(\"localhost\", 3000, null);zk.create(\"/metrics\", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL);zk.create(\"/metrics\", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL);List children = zk.getChildren(\"/\", null);System.out.println(children);// [metrics0000000001, metrics0000000002] 使用上面的代码就能在 Zookeeper 中创建两个带序号的 metrics 节点，分别是 metrics0000000001 和 metrics0000000002，也就是说 Zookeeper 帮助我们保证了节点的唯一性，让我们能通过唯一的 ID 查找到对应服务的地址等信息。 协调分布式事务Zookeeper 的另一个作用就是担任分布式事务中的协调者角色，在之前介绍 分布式事务 的文章中我们曾经介绍过分布式事务本质上都是通过 2PC 来实现的，在两阶段提交中就需要一个协调者负责协调分布式事务的执行。 123456789ZooKeeper zk = new ZooKeeper(\"localhost\", 3000, null);String path = zk.create(\"/transfer/tx\", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL);List ops = Arrays.asList( Op.create(path + \"/cohort\", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL), Op.create(path + \"/cohort\", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL), Op.create(path + \"/cohort\", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL));zk.multi(ops); 当前节点作为协调者在每次发起分布式事务时都会创建一个 /transfer/tx 的持久顺序节点，然后为几个事务的参与者创建几个空白的节点，事务的参与者在收到事务时会向这些空白的节点中写入信息并监听这些节点中的内容。 所有的事务参与者会向当前节点中写入提交或者终止，一旦当前的节点改变了事务的状态，其他节点就会得到通知，如果出现一个写入终止的节点，所有的节点就会回滚对分布式事务进行回滚。 使用 Zookeeper 实现强一致性的分布式事务其实还是一件比较困难的事情，一方面是因为强一致性的分布式事务本身就有一定的复杂性，另一方面就是 Zookeeper 为了给客户端提供更多的自由，对外暴露的都是比较基础的 API，对它们进行组装实现复杂的分布式事务还是比较麻烦的，对于如何使用 Zookeeper 实现分布式事务，我们可以在 ZooKeeper Recipes and Solutions 一文中找到更为详细的内容。 分布式锁在数据库中，锁的概念其实是非常重要的，常见的关系型数据库就会对排他锁和共享锁进行支持，而 Zookeeper 提供的 API 也可以让我们非常简单的实现分布式锁。 1234567891011121314151617181920212223242526272829ZooKeeper zk = new ZooKeeper(\"localhost\", 3000, null);final String resource = \"/resource\";final String lockNumber = zk .create(\"/resource/lock-\", null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);List&lt;String&gt; locks = zk.getChildren(resource, false, null);Collections.sort(locks);if (locks.get(0).equals(lockNumber.replace(\"/resource/\", \"\"))) &#123; System.out.println(\"Acquire Lock\"); zk.delete(lockNumber, 0);&#125; else &#123; zk.getChildren(resource, new Watcher() &#123; public void process(WatchedEvent watchedEvent) &#123; try &#123; ZooKeeper zk = new ZooKeeper(\"localhost\", 3000, null); List locks = zk.getChildren(resource, null, null); Collections.sort(locks); if (locks.get(0).equals(lockNumber.replace(\"/resource/\", \"\"))) &#123; System.out.println(\"Acquire Lock\"); zk.delete(lockNumber, 0); &#125; &#125; catch (Exception e) &#123;&#125; &#125; &#125;, null);&#125; 如果多个服务同时要对某个资源进行修改，就可以使用上述的代码来实现分布式锁，假设集群中存在一个资源 /resource，几个服务需要通过分布式锁保证资源只能同时被一个节点使用，我们可以用创建临时顺序节点的方式实现分布式锁；当我们创建临时节点后，通过 getChildren 获取当前等待锁的全部节点，如果当前节点是所有节点中序号最小的就得到了当前资源的使用权限，在对资源进行处理后，就可以通过删除 /resource/lock-00000000x 来释放锁，如果当前节点不是最小值，就会注册一个 Watcher 等待 /resource 子节点的变化直到当前节点的序列号成为最小值。 上述代码在集群中争夺同一资源的服务器特别多的情况下会出现羊群效应，每次子节点改变时都会通知当前节点，造成资源的浪费，我们其实可以将 getChildren 换成 getData，让当前节点只监听前一个节点的删除事件： 1234567891011121314Integer number = Integer.parseInt(lockNumber.replace(\"/resource/lock-\", \"\")) + 1;String previousLock = \"/resource/lock-\" + String.format(\"%010d\", number);zk.getData(previousLock, new Watcher() &#123; public void process(WatchedEvent watchedEvent) &#123; try &#123; if (watchedEvent.getType() == Event.EventType.NodeDeleted) &#123; System.out.println(\"Acquire Lock\"); ZooKeeper zk = new ZooKeeper(\"localhost\", 3000, null); zk.delete(lockNumber, 0); &#125; &#125; catch (Exception e) &#123;&#125; &#125;&#125;, null); 在新的分布式锁实现中，我们减少了每一个服务需要关注的事情，只让它们监听需要关心的数据变更，减少 Zookeeper 发送不必要的通知影响效率。 分布式锁作为分布式系统中比较重要的一个工具，确实有着比较多的应用，同时也有非常多的实现方式，除了 Zookeeper 之外，其他服务例如 Redis 和 etcd 也能够实现分布式锁，为分布式系统的构建提供支持，不过在这篇文章中就不展开介绍了。 总结我们在这篇文章中简单介绍了 Google 的分布式锁服务 Chubby 以及同样能够提供分布式锁服务功能的 Zookeeper。 作为分布式协调服务，Zookeeper 的应用场景非常广泛，不仅能够用于服务配置的下发、命名服务、协调分布式事务以及分布式锁，还能够用来实现微服务治理中的服务注册以及发现等功能，这些其实都源于 Zookeeper 能够提供高可用的分布式协调服务，能够为客户端提供分布式一致性的支持，在后面的文章中作者也会介绍其他用于分布式协调的服务。 参考资料https://zookeeper.apache.org/doc/r3.4.4/recipes.html https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf","categories":[],"tags":[]},{"title":"Shadowsocks服务器代理","slug":"Shadowsocks服务器代理","date":"2018-07-13T14:58:47.000Z","updated":"2018-07-13T15:11:09.791Z","comments":true,"path":"2018/07/13/Shadowsocks服务器代理/","link":"","permalink":"https://vincentruan.github.io/2018/07/13/Shadowsocks服务器代理/","excerpt":"","text":"前言本文讲述如何安装配置Shadowsocks Server，并支持通过代理方式（http/sock v4/5)连接因特网. 这里主要阐述服务端通过proxy连接的解决方案，shadowsocks server直连网络的方式比较简单，网上这块资料也比较齐全，不做过多描述， ssserver代理安装配置安装Shadowsocks Server参考Install Shadowsocks Server on Windows， 客户端的安装方式参考Shadowsocks Client安装, 这里主要解决服务端通过代理解决shadowsocks server无法直连网络的问题，客户端这块不做过多描述。 更新代理脚本​ 这个问题的解决方案来自github的一个issue 通过猴子补丁的方式给ss添加了一个前置代理的功能 有兴趣深入了解的推荐star一下该作者的项目PySocket ​ 在上述步骤安装了python版的Shadowsocks Server之后，通过猴子补丁的方式给给 shadowsocks 服务端添加前置代理的功能（原则上也适用于客户端），支持 http、socks4、socks5 代理。并且通过 hook 的方式去掉了ss的dns查询，ss在接收到数据之后会直接把域名和请求一起发给代理。 使用的时候修改 socket.py 文件中 PROXY_TYPE、PROXY_ADDR、PROXY_PORT 等字段为你的代理地址，然后把 socket.py 文件放到 shadowsocks 根目录即可生效，不用修改任何源码。 通过pip安装的话要放到ssserver所在的目录，一般都在 Python27\\Scripts （python27上验证OK） 12pip install win_inet_pton --proxy=http://your-proxy-host:your-proxy-portpip install shadowsocks --proxy=http://your-proxy-host:your-proxy-port 配置部分： 1234# the proxy type. SOCKS5 SOCKS4 HTTPPROXY_TYPE = SOCKS5PROXY_ADDR = \"127.0.0.1\"PROXY_PORT = 1080 socket.py 文末部分，因为我选择 hook shadowsocks的代码，实际使用时在del module会报异常，因此将文末修改为 12345678910111213# hook shadowsocks's code remove the dns reqdef new_resolve(self, hostname, callback): callback((hostname, hostname), None)modules_list = [\"shadowsocks.common\", \"shadowsocks.shell\"]for x in modules_list: try: del sys.modules[x] except KeyError: print \"Error: key\", x, \"not found\"import shadowsocks.asyncdnsshadowsocks.asyncdns.DNSResolver.resolve = new_resolve 如果不想 hook shadowsocks的代码的话，把文件中末尾的代码删除即可，原文件代码末尾如下: 12345678910# hook shadowsocks's code remove the dns reqdef new_resolve(self, hostname, callback): callback((hostname, hostname), None)modules_list = [\"shadowsocks.common\", \"shadowsocks.shell\"]for x in modules_list: del sys.modules[x]import shadowsocks.asyncdnsshadowsocks.asyncdns.DNSResolver.resolve = new_resolve ssserver配置参考Configuration via Config File 创建一个配置文件 /etc/shadowsocks.json. 示例如下: 12345678910&#123; \"server\":\"my_server_ip\", \"server_port\":8388, \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"password\":\"mypassword\", \"timeout\":300, \"method\":\"aes-256-cfb\", \"fast_open\": false&#125; 配置文件字段详解: Name Explanation server ssserver监听地址，0.0.0.0监听本地所有网卡地址 server_port ssserver服务端口 local_address 本地监听地址 local_port 本地端口 password 用于加密的密码 timeout 超时设置，单位秒，不建议太长 method 默认: “aes-256-cfb”, 详见 Encryption fast_open 是否使用 TCP_FASTOPEN, true / false workers worker数量, 仅在Unix/Linux生效 在控制台中执行，日志直接显示在控制台，首次测试使用建议该方式，可通过ctrl+C退出: 1ssserver -c /etc/shadowsocks.json 后台静默执行: 1234# 启动服务ssserver -c /etc/shadowsocks.json -d start# 停止服务ssserver -c /etc/shadowsocks.json -d stop","categories":[{"name":"developer tools","slug":"developer-tools","permalink":"https://vincentruan.github.io/categories/developer-tools/"}],"tags":[{"name":"Shadowsocks","slug":"Shadowsocks","permalink":"https://vincentruan.github.io/tags/Shadowsocks/"}]},{"title":"docker碎片拾遗","slug":"docker碎片拾遗","date":"2018-06-24T05:22:34.000Z","updated":"2018-06-24T07:49:15.160Z","comments":true,"path":"2018/06/24/docker碎片拾遗/","link":"","permalink":"https://vincentruan.github.io/2018/06/24/docker碎片拾遗/","excerpt":"","text":"进入shell环境12docker psdocker exec -it &lt;container&gt; bash and run 12apt-get updateapt-get install vim ！不要去改系统配置正常运行的docker先保存一下docker的ID，之后不要去改下面的配置，否则docker会更新为新的那个，导致数据丢失","categories":[{"name":"docker","slug":"docker","permalink":"https://vincentruan.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://vincentruan.github.io/tags/docker/"}]},{"title":"Xshell显示X11图形化界面","slug":"Xshell显示X11图形化界面","date":"2018-06-17T15:10:35.000Z","updated":"2018-06-17T15:38:38.635Z","comments":true,"path":"2018/06/17/Xshell显示X11图形化界面/","link":"","permalink":"https://vincentruan.github.io/2018/06/17/Xshell显示X11图形化界面/","excerpt":"","text":"安装Xmanager全家桶使用前检查一下是否安装了Xshell、Xstart、Xmanager - Passive，正常安装Xmanager全家桶应该是全的 使用XStart登录通过SSH的方式尝试登录VPS， 正常成功后会这样提示 当然更多的可能是弹出个错误框提示“已拒绝X11转移申请”，这是因为默认的VPS一般不会安装XAUTH导致， 1sudo yum install xorg-x11-xauth 这里可能会缺一些其他组件，见招拆招即可，谷歌或者百度解决 设置XSHELL 打开会话对话框 选择要激活X11转发功能的会话 点击[属性]按钮 在[类别]中选择[连接-&gt;SSH-&gt;隧道] 选择[转发X11连接到] 如用户的PC上已安装Xmanager，请勾选[Xmanager(M)]。如使用其他PC X 服务器，请选择[X DISPLAY(D)]后输入适用的DISPLAY 点击[确定] 检查当前监听端口IMPORTANT 1sudo netstat -tnlp|grep sshd 注意上面监听的6010，Xmanager会把X DISPLAY选项自动查找为Xshell。其他 PC X 服务器程序需由用户进行设置。如果PC X 服务器使用TCP 6000号端口，DISPLAY设置为“localhost:0.0” ，也就是说，X11的偏移量是6000，因此下面需要设置一个最终要的DISPLAY的值:10.0，如下 123export DISPLAY=:10.0或者export DISPLAY=localhost:10.0 测试X11 DISPLAY如果本地已经有需要X11界面展示的应用，直接运行查看即可，如无，推荐使用xclock检查是否生效[以下步骤不是必须，自行选择] 1sudo yum install xclock 这里可能出现乱码之类的，可能需要安装x窗口相关包，和字体显示包 1sudo yum groupinstall \"X Window System\" \"Fonts\" 然后执行xclock，看是否在PC桌面显示对应的时钟图形。如果xclock出现Warning: Missing charsets in String to FontSet conversion，可以执行下面执行，然后重新执行 1export LC_ALL=C","categories":[{"name":"Linux","slug":"Linux","permalink":"https://vincentruan.github.io/categories/Linux/"}],"tags":[{"name":"X11 Display","slug":"X11-Display","permalink":"https://vincentruan.github.io/tags/X11-Display/"},{"name":"XSHELL","slug":"XSHELL","permalink":"https://vincentruan.github.io/tags/XSHELL/"}]},{"title":"Hexo添加Gitalk评论插件","slug":"Hexo添加Gitalk评论插件","date":"2018-06-01T15:55:56.000Z","updated":"2018-06-01T16:18:58.699Z","comments":true,"path":"2018/06/01/Hexo添加Gitalk评论插件/","link":"","permalink":"https://vincentruan.github.io/2018/06/01/Hexo添加Gitalk评论插件/","excerpt":"","text":"安装Gitalk提供了两种方式： 直接引入 1234567&lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css\"&gt;&lt;script src=\"https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js\"&gt;&lt;/script&gt;&lt;!-- or --&gt;&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/gitalk/dist/gitalk.css\"&gt;&lt;script src=\"https://unpkg.com/gitalk/dist/gitalk.min.js\"&gt;&lt;/script&gt; npm安装 123npm i --save gitalkimport &apos;gitalk/dist/gitalk.css&apos;import Gitalk from &apos;gitalk&apos; 相对来说第一种会更简单。 使用A GitHub Application is needed for authorization, if you don’t have one, Click here to register a new one. Note: You must specify the website domain url in the Authorization callback URL field. 1234567891011const gitalk = new Gitalk(&#123; clientID: &apos;GitHub Application Client ID&apos;, clientSecret: &apos;GitHub Application Client Secret&apos;, repo: &apos;GitHub repo&apos;, owner: &apos;GitHub repo owner&apos;, admin: [&apos;GitHub repo owner and collaborators, only these guys can initialize github issues&apos;], id: location.pathname, // Ensure uniqueness and length less than 50 distractionFreeMode: false // Facebook-like distraction free mode&#125;)gitalk.render(&apos;gitalk-container&apos;) 修改主题文件 这里以next主题为例，参考Feature: Add Gitalk Support 不同的主题目录和模板引擎不同，可以自己修改, 修改next主题配置文件_config.yml，添加字段： 12345678910# Gitalk# more info please open https://github.com/gitalk/gitalkgitalk: enable: false clientID: clientSecret: repo: owner: admin: # support multiple admins split with comma, e.g. foo,bar pagerDirection: first 找到next/layout/_third-party/comments文件夹，新建gitalk.swig文件，代码如下： 1234567891011121314151617181920&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname %&#125; &#123;% if theme.gitalk.enable %&#125; &#123;% if page.comments %&#125; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; const gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123;theme.gitalk.clientID&#125;&#125;&apos;, clientSecret: &apos;&#123;&#123;theme.gitalk.clientSecret&#125;&#125;&apos;, repo: &apos;&#123;&#123;theme.gitalk.repo&#125;&#125;&apos;, owner: &apos;&#123;&#123;theme.gitalk.owner&#125;&#125;&apos;, admin: &apos;&#123;&#123;theme.gitalk.admin&#125;&#125;&apos;.split(&apos;,&apos;), pagerDirection: &apos;&#123;&#123;theme.gitalk.pagerDirection&#125;&#125;&apos;, // facebook-like distraction free mode distractionFreeMode: false &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt; &#123;% endif %&#125; &#123;% endif %&#125;&#123;% endif %&#125; 同目录下在index.swig文件末尾添加： 1&#123;% include &apos;gitalk.swig&apos; %&#125; 下步搞起，next/layout/_partials文件夹下，找到comments.swig文件，添加代码： 123&#123;% elseif theme.gitalk.enable %&#125; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; 因为github限制了issue的提交title长度不能超过50，可能会遇到Error: Validation Failed 按照这里的方案，使用MD5的方式降低长度即可 参考文档 Hexo添加Gitalk评论插件 Next 第三方服务集成 在hexo next主题上使用gitalk","categories":[{"name":"Hexo使用攻略","slug":"Hexo使用攻略","permalink":"https://vincentruan.github.io/categories/Hexo使用攻略/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://vincentruan.github.io/tags/hexo/"}]},{"title":"Hexo集成Algolia搜索插件","slug":"Hexo集成Algolia搜索插件","date":"2018-06-01T15:14:36.000Z","updated":"2018-06-01T15:17:31.107Z","comments":true,"path":"2018/06/01/Hexo集成Algolia搜索插件/","link":"","permalink":"https://vincentruan.github.io/2018/06/01/Hexo集成Algolia搜索插件/","excerpt":"","text":"本文转载自Hexo集成Algolia搜索插件 前言个人博客自从2016年10月21日搭建以来，迄今为止已经有49 篇日志了。虽然不是很多篇文章，但是搜索站内的内容已经力不从心了。 搜索了网上很多关于“Hexo 站内搜索”的内容，发现大部分都是使用Swiftype，但是发现Swiftype 搜索只有15 天的免费，之后就需要开始收费了。 因为只是为自己的个人博客 使用站内搜索，所以希望找一个类似与Swiftype 的，但是免费的站内搜索。最后找了Algolia 这个免费版本替代。 下面简单说下搭建过程： 搭建过程前提条件如果你的Next 版本为5.1.0 之后，可以使用Algolia。如果不是，请先升级到5.1.0 版本之后 一个Algolia 帐号官网地址 使用GitHub 或Google 帐号登录。 创建Index进入Dashboard，选择Indices 新建一个Index。 安装Hexo AlgoliaIndex 创建完成后，此时Index 为包含任何数据。需要安装Hexo Aloglia 扩展，这个扩展的功能是搜集站点的内容并通过API 发送给Aloglia。前往站点根目录，执行命令安装： 1npm install --save hexo-algolia 获取Key，更新站点信配置点击Dashborad 左侧的API Keys，其中的信息接下来将会被用到。包括Application ID 、Search-Only API Key 和 Admin API Key，其中Admin API Key需要保密保存 编辑站点配置文件，新增以下配置： 123456algolia: applicationID: &apos;SV57WJ53OS&apos; apiKey: &apos;c7d219504e44d09ab55f5f7a195fce98&apos; adminApiKey: &apos;adminApiKey&apos; indexName: &apos;dev_jobbymsblog&apos; chunkSize: 5000 更新Index当配置完成，在站点根目录下执行hexo algolia 来更新Index。请注意观察命令的输出。 主题集成更改主题配置文件，找到Algolia Search 配置部分： 123456789# Algolia Searchalgolia_search: enable: false hits: per_page: 10 labels: input_placeholder: Search for Posts hits_empty: &quot;We didn&apos;t find any results for the search: $&#123;query&#125;&quot; hits_stats: &quot;$&#123;hits&#125; results found in $&#123;time&#125; ms&quot; 将enable 改为true 即可，根据需要你可以调整labels 中的文本。 问题11. 点击搜索结果，结果跳转地址为： 1Cannot GET /undefined/ 按照5.1.0使用algolia搜索问题这里进行的处理，在这里总结一下： 因为hexo-aloglia 的作者没有把post.path 加入index，所以data.path 是undefined。 遇到这个问题，首先运行npm uninstll hexo-algolia 卸载之前的版本，再运行npm install hexo-algolia@0.2.0 --save,最后运行hexo algolia 命令重新index 就可以了。 参考文档 Swiftype站内搜索 Next 第三方服务集成","categories":[{"name":"Hexo使用攻略","slug":"Hexo使用攻略","permalink":"https://vincentruan.github.io/categories/Hexo使用攻略/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://vincentruan.github.io/tags/Hexo/"},{"name":"Algolia","slug":"Algolia","permalink":"https://vincentruan.github.io/tags/Algolia/"}]},{"title":"Linux xmr-stak挖矿教程","slug":"Linux-xmr-stak挖矿教程","date":"2018-05-27T13:51:10.000Z","updated":"2018-06-08T17:29:23.624Z","comments":true,"path":"2018/05/27/Linux-xmr-stak挖矿教程/","link":"","permalink":"https://vincentruan.github.io/2018/05/27/Linux-xmr-stak挖矿教程/","excerpt":"","text":"在Linux上编译 xmr-stakInstall DependenciesAMD APP SDK 3.0 (only needed to use AMD GPUs) download and install the latest version from https://www.dropbox.com/sh/mpg882ekirnsfa7/AADWz5X-TgVdsmWt0QwMgTWLa/AMD-APP-SDKInstaller-v3.0.130.136-GA-linux64.tar.bz2?dl=0(do not wonder why it is a link to a dropbox but AMD has removed the SDK downloads, see https://community.amd.com/thread/228059) Cuda 8.0+ (only needed to use NVIDIA GPUs) download and install https://developer.nvidia.com/cuda-downloads for minimal install choose Custom installation options during the install and select CUDA/Develpment CUDA/Runtime Driver components GNU Compiler12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# Ubuntu / Debiansudo apt install libmicrohttpd-dev libssl-dev cmake build-essential libhwloc-devgit clone https://github.com/fireice-uk/xmr-stak.gitmkdir xmr-stak/buildcd xmr-stak/buildcmake ..make install# Archsudo pacman -S --needed base-devel hwloc openssl cmake libmicrohttpdgit clone https://github.com/fireice-uk/xmr-stak.gitmkdir xmr-stak/buildcd xmr-stak/buildcmake ..make install# Fedorasudo dnf install gcc gcc-c++ hwloc-devel libmicrohttpd-devel libstdc++-static make openssl-devel cmakegit clone https://github.com/fireice-uk/xmr-stak.gitmkdir xmr-stak/buildcd xmr-stak/buildcmake ..make install# CentOSsudo yum install centos-release-scl epel-releasesudo yum install cmake3 devtoolset-4-gcc* hwloc-devel libmicrohttpd-devel openssl-devel makescl enable devtoolset-4 bashgit clone https://github.com/fireice-uk/xmr-stak.gitmkdir xmr-stak/buildcd xmr-stak/buildcmake3 ..make install# Ubuntu 14.04sudo add-apt-repository ppa:ubuntu-toolchain-r/testsudo apt updatesudo apt install gcc-5 g++-5 makesudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 1 --slave /usr/bin/g++ g++ /usr/bin/g++-5curl -L http://www.cmake.org/files/v3.4/cmake-3.4.1.tar.gz | tar -xvzf - -C /tmp/cd /tmp/cmake-3.4.1/ &amp;&amp; ./configure &amp;&amp; make &amp;&amp; sudo make install &amp;&amp; cd -sudo update-alternatives --install /usr/bin/cmake cmake /usr/local/bin/cmake 1 --forcesudo apt install libmicrohttpd-dev libssl-dev libhwloc-devgit clone https://github.com/fireice-uk/xmr-stak.gitmkdir xmr-stak/buildcd xmr-stak/buildcmake ..make install# TinyCore Linux 8.x# TinyCore is 32-bit only, but there is an x86-64 port, known as &quot;Pure 64,&quot;# hosted on the TinyCore home page, and it works well.# Beware that huge page support is not enabled in the kernel distributed# with Pure 64. Consider http://wiki.tinycorelinux.net/wiki:custom_kernel# Note that as of yet there are no distro packages for microhttpd or hwloc.# hwloc is easy enough to install manually though, shown below.# Also note that only CPU mining has been tested on this platform, thus the# disabling of CUDA and OpenCL shown below.tce-load -iw openssl-dev.tcz cmake.tcz make.tcz gcc.tcz git.tcz \\ glibc_base-dev.tcz linux-4.8.1_api_headers.tcz \\ glibc_add_lib.tczwget https://www.open-mpi.org/software/hwloc/v1.11/downloads/hwloc-1.11.8.tar.gztar xzvf hwloc-1.11.8.tar.gzcd hwloc-1.11.8./configure --prefix=/usr/localmakesudo make installcd ..git clone http://github.com/fireice-uk/xmr-stakcd xmr-stakmkdir buildcd buildCC=gcc cmake .. -DCUDA_ENABLE=OFF \\ -DOpenCL_ENABLE=OFF \\ -DMICROHTTPD_ENABLE=OFFmake install g++ version 5.1 or higher is required for full C++11 support.If you want to compile the binary without installing libraries / compiler or just compile binary for some other distribution, please check the build_xmr-stak_docker.sh script. Some newer gcc versions are not supported by CUDA (e.g. Ubuntu 17.10). It will require installing gcc 5 but you can avoid changing defaults. In that case you can force CUDA to use an older compiler in the following way:1cmake -DCUDA_HOST_COMPILER=/usr/bin/gcc-5 .. To do a generic and static build for a system without gcc 5.1+1234cmake -DCMAKE_LINK_STATIC=ON -DXMR-STAK_COMPILE=generic .make installcd bin\\Releasecopy C:\\xmr-stak-dep\\openssl\\bin\\* . Note - cmake caches variables, so if you want to do a dynamic build later you need to specify ‘-DCMAKE_LINK_STATIC=OFF’ Reference xmr-stak","categories":[{"name":"block-chain","slug":"block-chain","permalink":"https://vincentruan.github.io/categories/block-chain/"}],"tags":[{"name":"xmr","slug":"xmr","permalink":"https://vincentruan.github.io/tags/xmr/"}]},{"title":"Day 1: Bower —— 管理你的客户端依赖关系","slug":"Day-1-Bower-——-管理你的客户端依赖关系","date":"2018-05-26T14:27:38.000Z","updated":"2018-05-26T14:40:10.462Z","comments":true,"path":"2018/05/26/Day-1-Bower-——-管理你的客户端依赖关系/","link":"","permalink":"https://vincentruan.github.io/2018/05/26/Day-1-Bower-——-管理你的客户端依赖关系/","excerpt":"","text":"编者注：我们发现了比较有趣的系列文章《30天学习30种新技术》，准备翻译，一天一篇更新，年终礼包。以下是第一天技术的译文。 我决定将第一天的学习主题选为Bower。 什么是Bower？Bower是一个客户端技术的软件包管理器，它可用于搜索、安装和卸载如JavaScript、HTML、CSS之类的网络资源。其他一些建立在Bower基础之上的开发工具，如YeoMan和Grunt，这个会在以后的文章中介绍。 为什么我会在意Bower？ 节省时间。为什么要学习Bower的第一个原因，就是它会为你节省寻找客户端的依赖关系的时间。每次我需要安装jQuery的时候，我都需要去jQuery网站下载包或使用CDN版本。但是有了Bower，你只需要输入一个命令，jquery就会安装在本地计算机上，你不需要去记版本号之类的东西，你也可以通过Bower的info命令去查看任意库的信息。 脱机工作。Bower会在用户主目录下创建一个.bower的文件夹，这个文件夹会下载所有的资源、并安装一个软件包使它们可以离线使用。如果你熟悉Java，Bower即是一个类似于现在流行的Maven构建系统的.m2仓库。每次你下载任何资源库都将被安装在两个文件夹中 —— 一个在的应用程序文件夹，另一个在用户主目录下的.bower文件夹。因此，下一次你需要这个仓库时，就会用那个用户主目录下.bower中的版本。 可以很容易地展现客户端的依赖关系。你可以创建一个名为bower.json的文件，在这个文件里你可以指定所有客户端的依赖关系，任何时候你需要弄清楚你正在使用哪些库，你可以参考这个文件。 让升级变得简单。假设某个库的新版本发布了一个重要的安全修补程序，为了安装新版本，你只需要运行一个命令，bower会自动更新所有有关新版本的依赖关系。 前提准备为了安装bower，你首先需要安装如下文件： Node：下载最新版本的node.js NPM：NPM是node程序包管理器。它是捆绑在nodejs的安装程序上的，所以一旦你已经安装了node，NPM也就安装好了。 Git：你需要从git仓库获取一些代码包。 安装Bower一旦你已经安装了上面所说的所有必要文件，键入以下命令安装Bower： 1$ npm install -g bower 这行命令是Bower的全局安装，-g 操作表示全局。 开始使用Bower安装完bower之后就可以使用所有的bower命令了。可以键入help 命令来查看bower可以完成那些操作，如下： 1234567891011121314151617181920212223242526272829303132$ bower helpUsage: bower &lt;command&gt; [&lt;args&gt;] [&lt;options&gt;]Commands: cache Manage bower cache help Display help information about Bower home Opens a package homepage into your favorite browser info Info of a particular package init Interactively create a bower.json file install Install a package locally link Symlink a package folder list List local packages lookup Look up a package URL by name prune Removes local extraneous packages register Register a package search Search for a package by name update Update a local package uninstall Remove a local packageOptions: -f, --force Makes various commands more forceful -j, --json Output consumable JSON -l, --log-level What level of logs to report -o, --offline Do not hit the network -q, --quiet Only output important information -s, --silent Do not output anything, besides errors -V, --verbose Makes output more verbose --allow-root Allows running commands as root 包的安装Bower是一个软件包管理器，所以你可以在应用程序中用它来安装新的软件包。举例来看一下来如何使用Bower安装JQuery，在你想要安装该包的地方创建一个新的文件夹，键入如下命令： 1$ bower install jquery 上述命令完成以后，你会在你刚才创建的目录下看到一个bower_components的文件夹，其中目录如下： 123456789101112131415$ tree bower_components/bower_components/└── jquery ├── README.md ├── bower.json ├── component.json ├── composer.json ├── jquery-migrate.js ├── jquery-migrate.min.js ├── jquery.js ├── jquery.min.js ├── jquery.min.map └── package.json1 directory, 10 files 包的使用现在就可以在应用程序中使用jQuery包了，在jQuery里创建一个简单的html5文件： 12345678910111213141516171819202122&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Learning Bower&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;Animate Me!!&lt;/button&gt;&lt;div style=&quot;background:red;height:100px;width:100px;position:absolute;&quot;&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;bower_components/jquery/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(document).ready(function()&#123; $(&quot;button&quot;).click(function()&#123; $(&quot;div&quot;).animate(&#123;left:&apos;250px&apos;&#125;); &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 正如你所看到的，你刚刚引用jquery.min.js文件，现阶段完成。 所有包的列表如果你想找出所有安装在应用程序中的包，可以使用list命令： 1234$ bower listbower check-new Checking for new versions of the project dependencies..blog /Users/shekhargulati/day1/blog└── jquery#2.0.3 extraneous 包的搜索假如你想在你的应用程序中使用twitter的bootstrap框架，但你不确定包的名字，这时你可以使用search 命令： 123456$ bower search bootstrapSearch results: bootstrap git://github.com/twbs/bootstrap.git angular-bootstrap git://github.com/angular-ui/bootstrap-bower.git sass-bootstrap git://github.com/jlong/sass-twitter-bootstrap.git 包的信息如果你想看到关于特定的包的信息，可以使用info 命令来查看该包的所有信息： 1234567891011121314151617181920212223242526272829$ bower info bootstrapbower bootstrap#* not-cached git://github.com/twbs/bootstrap.git#*bower bootstrap#* resolve git://github.com/twbs/bootstrap.git#*bower bootstrap#* download https://github.com/twbs/bootstrap/archive/v3.0.0.tar.gzbower bootstrap#* extract archive.tar.gzbower bootstrap#* resolved git://github.com/twbs/bootstrap.git#3.0.0&#123; name: &apos;bootstrap&apos;, version: &apos;3.0.0&apos;, main: [ &apos;./dist/js/bootstrap.js&apos;, &apos;./dist/css/bootstrap.css&apos; ], ignore: [ &apos;**/.*&apos; ], dependencies: &#123; jquery: &apos;&gt;= 1.9.0&apos; &#125;, homepage: &apos;https://github.com/twbs/bootstrap&apos;&#125;Available versions: - 3.0.0 - 3.0.0-rc1 - 3.0.0-rc.2 - 2.3.2 ..... 如果你想得到单个包的信息，也可以使用info 命令： 12345678910111213141516171819$ bower info bootstrap#3.0.0bower bootstrap#3.0.0 cached git://github.com/twbs/bootstrap.git#3.0.0bower bootstrap#3.0.0 validate 3.0.0 against git://github.com/twbs/bootstrap.git#3.0.0&#123; name: &apos;bootstrap&apos;, version: &apos;3.0.0&apos;, main: [ &apos;./dist/js/bootstrap.js&apos;, &apos;./dist/css/bootstrap.css&apos; ], ignore: [ &apos;**/.*&apos; ], dependencies: &#123; jquery: &apos;&gt;= 1.9.0&apos; &#125;, homepage: &apos;https://github.com/twbs/bootstrap&apos;&#125; 包的卸载卸载包可以使用uninstall 命令： 1$ bower uninstall jquery bower.json文件的使用bower.json文件的使用可以让包的安装更容易，你可以在应用程序的根目录下创建一个名为“bower.json”的文件，并定义它的依赖关系。使用bower init 命令来创建bower.json文件： 123456789101112131415161718192021222324252627282930313233$ bower init[?] name: blog[?] version: 0.0.1[?] description:[?] main file:[?] keywords:[?] authors: Shekhar Gulati &lt;shekhargulati84@gmail.com&gt;[?] license: MIT[?] homepage:[?] set currently installed components as dependencies? Yes[?] add commonly ignored files to ignore list? Yes[?] would you like to mark this package as private which prevents it from being accidentally published to the registry? No&#123; name: &apos;blog&apos;, version: &apos;0.0.1&apos;, authors: [ &apos;Shekhar Gulati &lt;shekhargulati84@gmail.com&gt;&apos; ], license: &apos;MIT&apos;, ignore: [ &apos;**/.*&apos;, &apos;node_modules&apos;, &apos;bower_components&apos;, &apos;test&apos;, &apos;tests&apos; ], dependencies: &#123; jquery: &apos;~2.0.3&apos; &#125;&#125;[?] Looks good? Yes 可以查看该文件： 123456789101112131415161718&#123; &quot;name&quot;: &quot;blog&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;authors&quot;: [ &quot;Shekhar Gulati &lt;shekhargulati84@gmail.com&gt;&quot; ], &quot;license&quot;: &quot;MIT&quot;, &quot;ignore&quot;: [ &quot;**/.*&quot;, &quot;node_modules&quot;, &quot;bower_components&quot;, &quot;test&quot;, &quot;tests&quot; ], &quot;dependencies&quot;: &#123; &quot;jquery&quot;: &quot;~2.0.3&quot; &#125;&#125; 注意看，它已经加入了jQuery依赖关系。 现在假设也想用twitter bootstrap，我们可以用下面的命令安装twitter bootstrap并更新bower.json文件： 1$ bower install bootstrap --save 它会自动安装最新版本的bootstrap并更新bower.json文件： 12345678910111213141516171819&#123; &quot;name&quot;: &quot;blog&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;authors&quot;: [ &quot;Shekhar Gulati &lt;shekhargulati84@gmail.com&gt;&quot; ], &quot;license&quot;: &quot;MIT&quot;, &quot;ignore&quot;: [ &quot;**/.*&quot;, &quot;node_modules&quot;, &quot;bower_components&quot;, &quot;test&quot;, &quot;tests&quot; ], &quot;dependencies&quot;: &#123; &quot;jquery&quot;: &quot;~2.0.3&quot;, &quot;bootstrap&quot;: &quot;~3.0.0&quot; &#125;&#125; 这就是今天的学习，希望能让你对bower有个足够的了解，最好可以自己尝试一下。 原文 Day 1: Bower–Manage Your Client Side Dependencies翻译 SegmentFault","categories":[{"name":"30天学习30种新技术系列","slug":"30天学习30种新技术系列","permalink":"https://vincentruan.github.io/categories/30天学习30种新技术系列/"}],"tags":[{"name":"30天学习30种新技术系列","slug":"30天学习30种新技术系列","permalink":"https://vincentruan.github.io/tags/30天学习30种新技术系列/"}]},{"title":"30天学习30种新技术系列","slug":"30-天学习-30-种新技术系列","date":"2018-05-26T14:27:27.000Z","updated":"2018-05-26T14:49:45.231Z","comments":true,"path":"2018/05/26/30-天学习-30-种新技术系列/","link":"","permalink":"https://vincentruan.github.io/2018/05/26/30-天学习-30-种新技术系列/","excerpt":"","text":"编者注：我们发现了比较有趣的系列文章《30 天学习 30 种新技术》，准备翻译，一天一篇更新，年终礼包。以下是译文，英文标题表示还未翻译，附原文链接；中文标题表示已翻译，附译文链接。 更新：全系列已经全部翻译完成。 让你 30 天学习 30 种新技术，你会觉得这是挑战吗？ 我已经接受了挑战，我会在一个月的时间内每天学习一门新技术，挑战开始于 2013 年 10 月 29 日。下面就是我将要学习的新技术的列表，我会把每天学到的内容写出来。在我每天正常的工作之后，我会花几个小时学习一门新技术，再用一小时将今天学到的写在博客上。这项活动的目的是熟悉许多在开发者社区所使用的新技术。 我会把重点放在 JavaScript 及其相关技术的学习上，当然也会去了解一下像 Java 这类我比较感兴趣的其他技术。我也可能会在一门技术上花费好几天的时间，但我每次会选择和这门技术相关的不同的主题来讲。只要是有意义的，我将尽量展示它如何与 OpenShift 工作，我希望这是一次充满乐趣并能学到很多东西的旅程。（你可以在 twitter 上follow 我） 下边是学习列表： 2013.10.29 - Day 1: Bower —— 管理你的客户端依赖关系 2013.10.30 - Day 2: AngularJS —— 对 AngularJS 的初步认识 2013.10.31 - Day 3: Flask —— 使用 Python 和 OpenShift 进行即时 Web 开发 2013.11.01 - Day 4: PredictionIO —— 如何创建一个博客推荐器 2013.11.02 - Day 5: GruntJS —— 重复乏味的工作总会有人做（反正我不做） 2013.11.03 - Day 6: 在 Java 虚拟机上使用 Grails 进行快速 Web 开发 2013.11.04 - Day 7: GruntJS 在线重载 提升生产率至新境界 2013.11.05 - Day 8: Harp.JS —— 现代静态 Web 服务器 2013.11.06 - Day 9: TextBlob —— 对文本进行情感分析 2013.11.07 - Day 10: PhoneGap —— 开发手机应用如此简单 2013.11.08 - Day 11: AeroGear 推送服务器：使应用的通知推送变得简单 2013.11.09 - Day 12: OpenCV —— Java 开发者的人脸检测 2013.11.10 - Day 13: Dropwizard —— 非常棒的 Java REST 服务器栈 2013.11.11 - Day14：使用斯坦福 NER 软件包实现你自己的命名实体识别器（Named Entity Recognition，NER） 2013.11.12 - Day 15：Meteor —— 从零开始创建一个 Web 应用 2013.11.13 - Day 16: Goose Extractor —— 好用的文章提取工具 2013.11.14 - Day 17: 使用 JBoss Forge 和 OpenShift 构建部署 JAVA EE 6 应用 2013.11.15 - Day 18: BoilerPipe —— Java开发者的文章提取工具 2013.11.16 - Day 19: EmberJS 入门指南 2013.11.17 - Day 20: 斯坦福CoreNLP —— 用 Java 给 Twitter 情感分析 2013.11.18 - Day 21：Docker 入门教程 2013.11.19 - Day 22： 使用 Spring、MongoDB 和 AngularJS 开发单页面应用 2013.11.20 - Day 23： 使用 TimelineJS 构建精美的时间轴 2013.11.21 - Day 24: 使用 Yeoman 自动构建 Ember 项目 2013.11.22 - Day 25: Tornado —— 联合 Tornado、MongoDB 和 AngularJS 进行应用开发 2013.11.23 - Day 26: TogetherJS —— 让我们一起来编程！ 2013.11.24 - Day 27: Restify —— 在Node.js中构建正确的REST Web服务 2013.11.25 - Day 28: OpenShift 的 Eclipse 集成 2013.11.26 - Day 29: 编写你的第一个 Google Chrome 扩展程序 2013.11.27 - Day 30: Play Framework —— Java 开发者的梦想框架 原文 Learning 30 Technologies in 30 Days: A Developer Challenge翻译 SegmentFault","categories":[{"name":"30天学习30种新技术系列","slug":"30天学习30种新技术系列","permalink":"https://vincentruan.github.io/categories/30天学习30种新技术系列/"}],"tags":[{"name":"30天学习30种新技术系列","slug":"30天学习30种新技术系列","permalink":"https://vincentruan.github.io/tags/30天学习30种新技术系列/"}]},{"title":"教你免费搭建个人博客，Hexo&Github","slug":"教你免费搭建个人博客，Hexo-Github","date":"2018-05-26T09:39:47.000Z","updated":"2018-05-26T10:11:28.716Z","comments":true,"path":"2018/05/26/教你免费搭建个人博客，Hexo-Github/","link":"","permalink":"https://vincentruan.github.io/2018/05/26/教你免费搭建个人博客，Hexo-Github/","excerpt":"","text":"什么是 Hexo？https://hexo.io/zh-cn/docs/ Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 一、配置Github首先注册、登录 https://github.com/ 记住自己的Username（很重要） 然后右上角选择 Create a new repository https://github.com/new Repository name （填自己的名字） yourname.github.io(yourname与你的注册用户名一致,这个就是你博客的域名了) 例如，我的域名是github.com/zhihuya，就填入zhihuya.github.io。成功后出现下面的画面 二、环境安装（node、git）1、安装 Node.js https://nodejs.org/en/ 2、安装 Git https://github.com/waylau/git-for-win Git教程 https://github.com/waylau/git-for-win廖雪峰老师的教程，非常好。 3、安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，名称和邮箱是Github上的 4、安装 Hexo。所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 1$ npm install -g hexo-cli （使用的cmder，超级好用~~。等待时间可能有点长） 好了到这一步我们环境全部安装好了。 三、设置在电脑F盘（自己随意）目录下新建文件夹 test，进入test，按住Shift键点击鼠标右键 因为我有安装Cmder，没有安装的点击“在此处打开命令窗口”，输入 1hexo init blog 稍微等待下，速度有点慢。成功提示 1INFO Start blogging with Hexo! 因为你初始化hexo 之后source目录下自带一篇hello world文章, 所以直接执行下方命令 123456$ hexo generate# 启动本地服务器$ hexo server# 在浏览器输入 http://localhost:4000/就可以看见网页和模板了INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 访问http://localhost:4000/，便可以看到网站初步的模样，不要激动，我们还要把网页发布到Github上去。 重新打开CMD，输入： 1ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; 一路Enter过来就好，得到信息： 1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 找到该文件，打开（sublime text），Ctrl + a复制里面的所有内容，然后进入Sign in to GitHub：https://github.com/settings/ssh New SSH key ——Title：blog —— Key：输入刚才复制的—— Add SSH key 四、配置博客在blog目录下，用sublime打开_config.yml文件，修改参数信息 特别提醒，在每个参数的：后都要加一个空格 修改网站相关信息 123456title: 崔斯特测试所用博客subtitle: 副标题description: 网页描述author: 崔斯特language: zh-CNtimezone: Asia/Shanghai 配置部署（我的是zhihuya，修改成自己的） 1234deploy: type: git repo: https://github.com/zhihuya/zhihuya.github.io.git branch: master 五、发表文章在CMD中输入 12$ hexo new &quot;崔斯特测试文章&quot;INFO Created: F:\\test\\blog\\source\\_posts\\崔斯特测试文章.md 找到该文章，打开，使用Markdown语法，该语法介绍可以查看https://zhangslob.github.io/2017/02/26/%E5%88%A9%E7%94%A8HEXO%E6%90%AD%E5%BB%BA%E7%9A%84%E5%8D%9A%E5%AE%A2/ 123456---title: 崔斯特测试文章date: 2017-02-28 13:03:44tags:---这是一篇测试文章，欢迎关注作者博客[1]: https://zhangslob.github.io/ 保存，然后执行下列步骤： 12345678910111213141516F:\\test\\blog$ hexo cleanINFO Deleted database.INFO Deleted public folder.F:\\test\\blog$ hexo generateINFO Start processingINFO Files loaded in 1.48 s#省略INFO 29 files generated in 4.27 sF:\\test\\blog$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 这个时候，打开http://localhost:4000/，发现刚才的文章已经成功了 最后一步，发布到网上，执行： 123456F:\\test\\blog$ hexo deployINFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...#省略 其中会跳出Github登录，直接登录，如果没有问题输入zhihuya（换成你的）.github.io/ 崔斯特测试所用博客https://zhihuya.github.io/ 然后就可以看到已经发布了 六、总结发布文章的步骤： 1、hexo new 创建文章 2、Markdown语法编辑文章 3、部署（所有打开CMD都是在blog目录下） 1234hexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo generate #生成hexo server #启动服务预览，非必要，可本地浏览网页hexo deploy #部署发布 简写Tips： hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo p == hexo publish hexo g == hexo generate#生成 hexo s == hexo server #启动服务预览 hexo d == hexo deploy#部署 如果在执行 hexo deploy 后,出现 error deployer not found:github 的错误，执行： 1npm install hexo-deployer-git --save 出错是正常的，出错了自己先百度或google，实在不知道的可以询问我。 托管的话不仅有github可以用，还有个国内的https://coding.net/可选 引用说明 作者：zhangslob 链接：https://zhangslob.github.io/2017/02/28/教你免费搭建个人博客，Hexo-Github","categories":[{"name":"Hexo使用攻略","slug":"Hexo使用攻略","permalink":"https://vincentruan.github.io/categories/Hexo使用攻略/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://vincentruan.github.io/tags/hexo/"},{"name":"个人博客搭建","slug":"个人博客搭建","permalink":"https://vincentruan.github.io/tags/个人博客搭建/"},{"name":"github","slug":"github","permalink":"https://vincentruan.github.io/tags/github/"}]},{"title":"Hexo Hello World","slug":"hexo-hello-world","date":"2018-05-26T09:16:29.000Z","updated":"2018-06-08T17:33:13.189Z","comments":true,"path":"2018/05/26/hexo-hello-world/","link":"","permalink":"https://vincentruan.github.io/2018/05/26/hexo-hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment hexo algolia12$ export HEXO_ALGOLIA_INDEXING_KEY=[algolia.apiKey]$ hexo algolia CI with jenkins使用Jenkins实现Hexo自动部署 hexo使用jenkins自动部署到阿里云 ###Cooperation 使用git clonegit@github.com:vincentruan/vincentruan.github.io.git拷贝仓库（git checkout -b hexo）； 在新拷贝的vincentruan.github.io文件夹下通过Git bash依次执行下列指令： npm install hexo-cli -g(首次安装)、npm install hexo、npm install、npm install hexo-deployer-git（记得，不需要hexo init这条指令,如果不慎在此时用了hexo init，则站点的配置文件_config.yml里面内容会被清空使用默认值，所以这一步一定要慎重 ）","categories":[{"name":"Hexo使用攻略","slug":"Hexo使用攻略","permalink":"https://vincentruan.github.io/categories/Hexo使用攻略/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://vincentruan.github.io/tags/hexo/"}]},{"title":"Hexo使用攻略-添加分类及标签","slug":"Hexo使用攻略-添加分类及标签","date":"2018-05-26T09:16:29.000Z","updated":"2018-06-01T16:19:04.056Z","comments":true,"path":"2018/05/26/Hexo使用攻略-添加分类及标签/","link":"","permalink":"https://vincentruan.github.io/2018/05/26/Hexo使用攻略-添加分类及标签/","excerpt":"","text":"1、创建“分类”选项1.1 生成“分类”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令 1$ hexo new page categories 成功后会提示： 1INFO Created: ~/Documents/blog/source/categories/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的： 1234---title: 文章分类date: 2017-05-27 13:47:40--- 添加type: &quot;categories&quot;到内容中，添加后是这样的： 123456---title: 文章分类date: 2017-05-27 13:47:40type: &quot;categories&quot;comments: false--- 保存并关闭文件。 1.2 给文章添加“categories”属性打开需要添加分类的文章，为其添加categories属性。下方的categories: web前端表示添加这篇文章到“web前端”这个分类。注意：hexo一篇文章只能属于一个分类，也就是说如果在“- web前端”下方添加“-xxx”，hexo不会产生两个分类，而是把分类嵌套（即该文章属于 “- web前端”下的 “-xxx ”分类）。 123456---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端--- 至此，成功给文章添加分类，点击首页的“分类”可以看到该分类下的所有文章。当然，只有添加了categories: xxx的文章才会被收录到首页的“分类”中。 2、创建“标签”选项2.1 生成“标签”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令 1$ hexo new page tags 成功后会提示： 1INFO Created: ~/Documents/blog/source/tags/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的： 1234---title: 标签date: 2017-05-27 14:22:08--- 添加type: &quot;tags&quot;到内容中，添加后是这样的： 123456---title: 文章分类date: 2017-05-27 13:47:40type: &quot;tags&quot;comments: false--- 保存并关闭文件。 2.2 给文章添加“tags”属性打开需要添加标签的文章，为其添加tags属性。下方的tags:下方的- jQuery - 表格 - 表单验证就是这篇文章的标签了 12345678910---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端tags:- jQuery- 表格- 表单验证--- 至此，成功给文章添加分类，点击首页的“标签”可以看到该标签下的所有文章。当然，只有添加了tags: xxx的文章才会被收录到首页的“标签”中。 细心的朋友可能已经发现，这两个的设置几乎一模一样！是的，没错，思路都是一样的。所以我们可以打开scaffolds/post.md文件，在tages:上面加入categories:,保存后，之后执行hexo new 文章名命令生成的文件，页面里就有categories:项了。 scaffolds目录下，是新建页面的模板，执行新建命令时，是根据这里的模板页来完成的，所以可以在这里根据你自己的需求添加一些默认值。","categories":[{"name":"Hexo使用攻略","slug":"Hexo使用攻略","permalink":"https://vincentruan.github.io/categories/Hexo使用攻略/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://vincentruan.github.io/tags/hexo/"}]},{"title":"Markdown吃了吗?","slug":"Markdown吃了吗","date":"2018-05-20T11:00:56.000Z","updated":"2018-05-26T09:38:15.933Z","comments":true,"path":"2018/05/20/Markdown吃了吗/","link":"","permalink":"https://vincentruan.github.io/2018/05/20/Markdown吃了吗/","excerpt":"","text":"markdown 介绍 Markdown 是一种轻量级标记语言，它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。 - wikipedia Daring Fireball: Markdown Project markdown Markdown wikipedia 介绍 MultiMarkdown 引入更多标记特性和输出选项的改进版Markdown why markdown 纯文本，兼容性极强，可以用任意文本编辑器打开. 语法简单（the syntax is so simple you can barely call it “syntax.”），零学习成本，极好的可读性，让你专注于文字写作而不是页面排版，并且兼容 HTML，simple but powerful . 格式转换方便，Markdown 的文本你可以轻松转换为 html、pdf、epub、电子书等。 适合团队协作，可以使用 git/svn 等进行版本控制管理。 阳志平：为什么 Markdown 成为科技界写作主流？ 图灵社区：用Markdown来写自由书籍-开源技术的方案 目前很多在线服务商均支持使用markdown编写： Github 最先支持，使用Markdown的一个分支版本来格式化评论、消息以及其它内容。 Stack Overflow 使用一种 Markdown 的分支作为它的文章格式化系统。 博客园 国内技术人的博客家园，每天活跃上万用户，高质量社区。 CSDN 号称全球最大中文IT社区，涵盖了多种语言、架构、博客、俱乐部等模块的技术论坛。 图灵社区 使用markdown语法供用户写作电子书. 简书 重拾文字的力量，交流故事，沟通想法，一个基于内容分享的社区。 为知笔记 国内顶尖笔记软件，支持使用Markdown语法编辑笔记。 有道云笔记 最新版本开始支持，并且支持一些扩展语法。 …… markdown 使用 Markdown: Basics （快速入门） Markdown 完整语法说明 (简体中文版) Github: Mastering Markdown GitHub 帮助中关于 Markdown 的语法帮助 MarkDown 语法团队规范 语法规范简洁版 Markdown Style Guide 语法规范复杂版 Markdown Cheatsheet GitHub Flavored Markdown GitHub 使用的 Markdown 语法，略微不同于标准 Markdown 语法。提供了一些更加简洁的语法，类似 URL autolinking, Strikethrough, Fenced code blocks, Syntax highlighting 等等 MultiMarkdown 介绍 对 markdown 进行的扩展功能 markdown 工具 马克飞象 web/chrome 离线客户端，markdown 全功能支持，最大特点内容能够同步到印象笔记（evernote）中，笔记的用户重度推荐，按年收费，目前作者 @weibo 正在开发跨平台的客户端。 StackEdit 在线 markdown 编辑器，可同步文档到Google Drive和 Dropbox，可发布文章到 Blogger，GitHub，Google Drive，Dropbox，Tumblr和WordPress。 cmd 作业部落 支持 win/mac/linux/web/chrome 全平台，支持实时同步预览，支持代码高亮、数学公式，区分写作和阅读模式，支持在线存储，分享文稿网址。 MacDown OSX 上的 Markdown 开源编辑器，支持代码高亮，实时预览等。 MarkdownPad Windows上的全功能Markdown编辑器，推荐win上使用，基本全部功能。 Marked2 多种 md 显示方案，不能够编辑文件，只用来展示文件，配合 subline text markdown edit 插件，完美使用； MWeb 专业的 Markdown 写作、记笔记、静态博客生成软件，由国内独立开发者@oulvhai开发，支持Toc、Table、代码高亮、支持发布到 Wordrpess 博客、支持 Metaweblog API 的博客服务、Wordpress.com、Evernote 和印象笔记、Blogger、Scriptogr.am、Tumblr等服务。 Haroopad 又一款简洁多功能的跨平台编辑器，全功能支持，再加上对社交网络友好的连接，多种主题等，感兴趣的可以看看。详情参考issue#1 Typora 不分栏，实时展示看到写出的内容，对于不喜欢「两栏」设计的人来说是一个选择 MarkEditor - ME MarkEditor以markdown为基础语法，多标签栏、文件夹结构，纯文本的方式带来优雅、高效的体验。 确实很棒的工具，带来很多新鲜的理念，支持、重构、提升 markdown，加快写作的体验。具体可以查看几篇评测文章： 简洁与强大，从不是矛盾的事物：写作工具 MarkEditor 功能详解 不止是一款简单的码字工具：MarkEditor 进阶功能介绍 码字必备：18 款优秀的 Markdown 写作工具 | 2015 年度盘点 喜欢哪一款，就看你的了。 这几款就够了，多了就有选择症 …… reference 参考 Why Markdown? A two-minute explanation 简书：献给写作者的 Markdown 新手指南 Markdown simple world","categories":[],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://vincentruan.github.io/tags/markdown/"}]}]}